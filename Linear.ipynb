{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e592a0a-ba58-41d1-bda3-b18540c1f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requirements.txt\", \"r\") as config_file:\n",
    "    config_code = config_file.read()\n",
    "    exec(config_code)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03953245-b743-4e70-b5cd-af826b2177c2",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555ac5f-6c9e-47b6-a72a-e52758a9f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_finance = sqlite3.connect(database=\"data/specialedata.sqlite\")\n",
    "\n",
    "macro_predictors = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM macro_predictors\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"macro_\")\n",
    ")\n",
    "\n",
    "JKPFactors = (pd.read_sql_query(\n",
    "  sql=\"SELECT * FROM JKPFactors\",\n",
    "  con=tidy_finance,\n",
    "  parse_dates={\"month\"})\n",
    "  .add_prefix(\"jkp_factor_\")\n",
    ")\n",
    "JKPFactornames = JKPFactors.columns\n",
    "\n",
    "factors_ff3_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff3_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff3_\")\n",
    ")\n",
    "\n",
    "factors_ff5_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff5_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff5_\")\n",
    ")\n",
    "\n",
    "ff_carhart = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM ff_carhart\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"ff_carhart_\")\n",
    ")\n",
    "\n",
    "crsp_2000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_2000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_1500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_1000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_250 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_250\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_100 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_100\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_50 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_50\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "# Select amount of tickers in cross section!\n",
    "data_total = (crsp_100\n",
    "        .merge(JKPFactors,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"jkp_factor_month\")\n",
    "        .merge(macro_predictors,\n",
    "             how = \"left\", left_on = \"month\", right_on = \"macro_month\")\n",
    "        .merge(factors_ff5_monthly,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"factor_ff5_month\")\n",
    "         .merge(ff_carhart,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"ff_carhart_month\")\n",
    "        .assign(ret_excess=lambda x: x[\"ret\"] - x[\"factor_ff5_rf\"]) \n",
    "        .drop(columns=['ret', 'jkp_factor_month', 'macro_month', 'factor_ff5_month', 'ff_carhart_month','ff_carhart_rf','ff_carhart_mkt_excess','ff_carhart_smb','ff_carhart_hml'])\n",
    "        .dropna()\n",
    "       )\n",
    "\n",
    "# Make a dataframe for stock characteristics and factors\n",
    "macro_variables = data_total.filter(like=\"macro\").columns\n",
    "factor_variables = data_total.filter(like=\"jkp_factor\").columns\n",
    "macro_factors = data_total[macro_variables]\n",
    "factors = data_total[macro_variables].merge(data_total[factor_variables], left_index=True, right_index=True)\n",
    "char = data_total[['mktcap', 'mktcap_lag_1', 'mktcap_lag_3', 'mktcap_lag_6', 'mktcap_lag_12', 'mom_1', 'mom_3','mom_6', 'mom_12']]\n",
    "# List of tickers\n",
    "tickers = data_total['ticker'].unique()\n",
    "\n",
    "# Transform data\n",
    "column_combinations = list(product(macro_factors, char)) \n",
    "\n",
    "new_column_values = []\n",
    "for macro_column, char in column_combinations:\n",
    "    new_column_values.append(data_total[macro_column] * data_total[char])\n",
    "\n",
    "column_names = [\" x \".join(t) for t in column_combinations]\n",
    "new_columns = pd.DataFrame(dict(zip(column_names, new_column_values)))\n",
    "\n",
    "# New data set with added combinations\n",
    "data = pd.concat([data_total, new_columns], axis=1)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "  transformers=[\n",
    "    (\"scale\", StandardScaler(), \n",
    "    [col for col in data.columns \n",
    "      if col not in [\"ret_excess\", \"month\", \"ticker\"]])\n",
    "  ],\n",
    "  remainder=\"drop\",\n",
    "  verbose_feature_names_out=False\n",
    ")\n",
    "training_date = \"2017-07-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047b344-d141-4e13-aa66-fefff0c95016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_total.set_index([\"ticker\",\"month\"])\n",
    "cols = list(data.columns)\n",
    "cols.insert(0, cols.pop(cols.index('ret_excess')))\n",
    "data = data[cols]\n",
    "data = data.iloc[:,:167] # Ensure full rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40256c3-8180-4217-b3dd-0d081bf3c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    training_date = \"2017-07-01\"\n",
    "    # Step 1: Filter data until the training date\n",
    "    data_train = data[data.index.get_level_values('month') <= training_date]\n",
    "    data_test = data[data.index.get_level_values('month') > training_date]\n",
    "\n",
    "    # Step 2: Prepare exogenous and endogenous variables\n",
    "    exog_vars = data.columns[1:].tolist()\n",
    "    endog_vars = ['ret_excess']  # Corrected spelling\n",
    "    X_train = sm.add_constant(data_train[exog_vars])\n",
    "    y_train = data_train[endog_vars]\n",
    "    \n",
    "    X_test = data_test[exog_vars]\n",
    "    y_test = data_test[endog_vars]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "def R_oos(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test).flatten(), np.array(y_pred).flatten()\n",
    "    return 1 - (np.dot((y_test-y_pred),(y_test-y_pred)))/(np.dot(y_test,y_test))\n",
    "\n",
    "def estimate(data, X_train, y_train, X_test, y_test):\n",
    "    model = PooledOLS(y_train, X_train)\n",
    "    pooled_res = model.fit()\n",
    "    \n",
    "    data_forecast = data[data.index.get_level_values('month') > training_date]\n",
    "    exog_vars = data.columns[1:].tolist()\n",
    "    X_forecast = sm.add_constant(data_forecast[exog_vars])\n",
    "    y_fitted = pooled_res.fitted_values\n",
    "\n",
    "    y_pred = pooled_res.predict(X_forecast)\n",
    "    print(f'Out-of-sample R-squared: {round(R_oos(y_test,y_pred),4)}')\n",
    "    return y_pred, y_fitted\n",
    "\n",
    "def plot(data, y_pred, y_fitted, dataset_name):\n",
    "    training_date = \"2017-07-01\"\n",
    "    plt.figure(figsize=(8, 3))\n",
    "\n",
    "    # Sort the DataFrame by index\n",
    "    df_sorted = data.sort_index()\n",
    "    y_pred = y_pred.sort_index()\n",
    "    y_fitted = y_fitted.sort_index()\n",
    "    # Plot line plots first\n",
    "    for ticker in df_sorted.index.levels[0]:\n",
    "        data_ticker = df_sorted.loc[ticker]\n",
    "        plt.scatter(data_ticker.index.get_level_values('month'), data_ticker['ret_excess'], label=ticker, color='deepskyblue', s=5, zorder=10, marker='o')\n",
    "        plt.scatter(y_pred.loc[ticker].index.get_level_values('month'), y_pred.loc[ticker]['predictions'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "        plt.scatter(y_fitted.loc[ticker].index.get_level_values('month'), y_fitted.loc[ticker]['fitted_values'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "    \n",
    "    plt.axvspan(training_date, max(data.index.get_level_values('month')), color='gray', alpha=0.2)\n",
    "    plt.grid(color='lightgray', linewidth=0.5, alpha=0.5)\n",
    "    plt.title(f'{dataset_name.upper()}', size = 8)\n",
    "\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Excess Return')\n",
    "\n",
    "    # Save\n",
    "    save_dir = 'plots/predictions'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Number of tickers in dataset\n",
    "    tickers = len(data.index.get_level_values('ticker').unique())\n",
    "    # Save the plot\n",
    "    save_path = os.path.join(save_dir, f'{tickers}_plot_{dataset_name.lower()}.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee14958-883d-412c-a3d0-90885969fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d890198-30ae-435d-b3c8-4cbe75d25884",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_fitted = estimate(data, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1836a-7b7b-409c-9968-e6c584d3e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data, y_pred, y_fitted, dataset_name = 'POLS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d2af4-cb28-4327-9f58-b010d7db4c94",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7101cbe-6dec-4893-846b-f6eb559d4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regularization functions\n",
    "data = data_total.set_index([\"ticker\",\"month\"])\n",
    "cols = list(data.columns)\n",
    "cols.insert(0, cols.pop(cols.index('ret_excess')))\n",
    "data = data[cols]\n",
    "def train_test_split_regularization(data):\n",
    "    y = data['ret_excess']\n",
    "    X = data.drop(columns = ['ret_excess'])\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_array = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled_array, index = X.index, columns = X.columns)\n",
    "    # Training 80 pct.\n",
    "    training_date = \"2017-07-01\"\n",
    "    X_train = X_scaled[X.index.get_level_values('month') < training_date]\n",
    "    X_test = X_scaled[X.index.get_level_values('month') >= training_date]\n",
    "    y_train = y[y.index.get_level_values('month') < training_date]\n",
    "    y_test = y[y.index.get_level_values('month') >= training_date]\n",
    "    \n",
    "    n_features = X_train.shape[1] # Number of features\n",
    "    feature_names = data.drop(columns=['ret_excess']).columns.tolist() # Feature names\n",
    "    \n",
    "    n_features = X_train.shape[1] # Number of features\n",
    "    feature_names = data.drop(columns=['ret_excess']).columns.tolist() # Feature names\n",
    "    y_train_ = np.array(y_train).reshape(-1,1)\n",
    "    return X_train, X_test, y_train, y_test, y_train_, n_features, feature_names\n",
    "\n",
    "def optimal_alpha_EN(X_train,y_train_, l1_ratio):\n",
    "    alphas = np.logspace(-6, -4, 100)\n",
    "    \n",
    "    lm_model = ElasticNet(alpha=0.006,\n",
    "                          l1_ratio=l1_ratio,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=lm_model, param_grid={'alpha': alphas}, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train.values, y_train_)\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "    best_alpha_str = \"{:.10f}\".format(round(best_alpha, 10))\n",
    "    \n",
    "    # Print the rounded alpha value\n",
    "    if l1_ratio == 1: \n",
    "        print(f'Optimal Alpha (Lasso): {best_alpha_str}')\n",
    "    else:\n",
    "         print(f'Optimal Alpha (EN): {best_alpha_str}')\n",
    "    return best_alpha\n",
    "\n",
    "def optimal_alpha_Ridge(X_train,y_train_):\n",
    "    alphas = np.logspace(-6, 6, 100)\n",
    "    \n",
    "    lm_model = Ridge(alpha=0.006,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=lm_model, param_grid={'alpha': alphas}, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train.values, y_train_)\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "    best_alpha_str = \"{:.10f}\".format(round(best_alpha, 10))\n",
    "    \n",
    "    # Print the rounded alpha value\n",
    "    print(f'Optimal Alpha (Ridge): {best_alpha_str}')\n",
    "    return best_alpha\n",
    "\n",
    "def EN_predict(X_train, y_train_, X_test, y_test, l1_ratio, alpha_opt):\n",
    "    lm_model = ElasticNet(alpha=alpha_opt,\n",
    "                          l1_ratio=l1_ratio,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    \n",
    "    lm_fit = lm_model.fit(X_train.values, y_train_)\n",
    "    predictions = lm_fit.predict(X_test.values)\n",
    "    fitted = lm_fit.predict(X_train.values)\n",
    "    \n",
    "    y_pred = pd.DataFrame(predictions, index = y_test.index)\n",
    "    y_fitted = pd.DataFrame(fitted, index = y_train.index)\n",
    "    \n",
    "    y_pred.columns = ['predictions']\n",
    "    y_fitted.columns = ['fitted_values']\n",
    "    print(f'Out-of-sample R-squared =', round(R_oos(y_test, y_pred),4))\n",
    "\n",
    "    # Creating a binary matrix to store selected features\n",
    "    selected_features = pd.DataFrame(np.zeros((len(X_train.columns), 1)), index=X_train.columns, columns=['selected'])\n",
    "    selected_features.loc[lm_fit.coef_ != 0, 'selected'] = 1\n",
    "\n",
    "    return y_pred, y_fitted, selected_features\n",
    "\n",
    "\n",
    "def Ridge_predict(X_train, y_train_, X_test, y_test, alpha_opt):\n",
    "    lm_model = Ridge(alpha=alpha_opt,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    \n",
    "    lm_fit = lm_model.fit(X_train.values, y_train_)\n",
    "    predictions = lm_fit.predict(X_test.values)\n",
    "    fitted = lm_fit.predict(X_train.values)\n",
    "    \n",
    "    y_pred = pd.DataFrame(predictions, index = y_test.index)\n",
    "    y_fitted = pd.DataFrame(fitted, index = y_train.index)\n",
    "    \n",
    "    y_pred.columns = ['predictions']\n",
    "    y_fitted.columns = ['fitted_values']\n",
    "    print(f'Out-of-sample R-squared =', round(R_oos(y_test, y_pred),4))\n",
    "\n",
    "    # features DataFrame initialization\n",
    "    features = pd.DataFrame(np.zeros((len(X_train.columns), 1)), index=X_train.columns, columns=['selected'])\n",
    "    \n",
    "    # Create coef_df\n",
    "    coef_df = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': lm_fit.coef_.flatten()})\n",
    "    \n",
    "    # Filter selected features based on coefficient values\n",
    "    selected_features = coef_df.loc[np.abs(coef_df['Coefficient']) >= 0.002, 'Feature'].tolist()\n",
    "    \n",
    "    # Update 'selected' column in features DataFrame\n",
    "    features.loc[selected_features, 'selected'] = 1\n",
    "    selected_values = features.loc[features['selected'] > 0.002, 'selected']\n",
    "    return y_pred, y_fitted,features\n",
    "    \n",
    "def plot(data, y_pred, y_fitted, regularizor):\n",
    "    training_date = \"2017-07-01\"\n",
    "    plt.figure(figsize=(8, 3))\n",
    "\n",
    "    # Sort the DataFrame by index\n",
    "    df_sorted = data.sort_index()\n",
    "    y_pred = y_pred.sort_index()\n",
    "    y_fitted = y_fitted.sort_index()\n",
    "    # Plot line plots first\n",
    "    plt.ylim(-1, 3)\n",
    "    for ticker in df_sorted.index.levels[0]:\n",
    "        data_ticker = df_sorted.loc[ticker]\n",
    "        plt.scatter(data_ticker.index.get_level_values('month'), data_ticker['ret_excess'], label=ticker, color='deepskyblue', s=5, zorder=10, marker='o')\n",
    "        plt.scatter(y_pred.loc[ticker].index.get_level_values('month'), y_pred.loc[ticker]['predictions'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "        plt.scatter(y_fitted.loc[ticker].index.get_level_values('month'), y_fitted.loc[ticker]['fitted_values'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "    \n",
    "    plt.axvspan(training_date, max(data.index.get_level_values('month')), color='gray', alpha=0.2)\n",
    "    plt.grid(color='lightgray', linewidth=0.5, alpha=0.5)\n",
    "    plt.title(f'{regularizor.upper()}', size = 8)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Excess Return')\n",
    "\n",
    "    # Save\n",
    "    save_dir = 'plots/predictions'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Number of tickers in dataset\n",
    "    tickers = len(data.index.get_level_values('ticker').unique())\n",
    "    # Save the plot\n",
    "    save_path = os.path.join(save_dir, f'{tickers}_plot_{regularizor}.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5c203-bcec-426c-9d51-6b4465aced14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05ee9b-a9b1-4c17-b0c5-973610379a58",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ea12f-33e9-4ad3-a81a-d857e35c9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_opt_lasso = optimal_alpha_EN(X_train,y_train_,1)\n",
    "y_pred_lasso, y_fitted_lasso, selected_features_lasso = EN_predict(X_train, y_train_, X_test,y_test, 1, alpha_opt_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e789484-fb12-471d-b06e-b3c72ed7cbba",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe5273-59f1-4456-b917-be9b05d096cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_opt_ridge = optimal_alpha_Ridge(X_train,y_train_)\n",
    "y_pred_ridge, y_fitted_ridge, selected_features_ridge = Ridge_predict(X_train, y_train_, X_test,y_test, alpha_opt_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a04b98b-26e1-47ba-99fb-83fac783cfc3",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81d948-4190-4570-b4da-9c4cd25e3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_opt_en = optimal_alpha_EN(X_train,y_train_,0.5)\n",
    "y_pred_en, y_fitted_en, selected_features_en = EN_predict(X_train, y_train_, X_test,y_test, 0.5, alpha_opt_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f39b74-b386-4ae5-b66f-c85ef3af6b37",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ee54e-387e-4919-8963-edac12e1c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lasso, y_fitted_lasso, selected_features_lasso = EN_predict(X_train, y_train_, X_test,y_test, 1, 0.0000532452)\n",
    "y_pred_ridge, y_fitted_ridge, selected_features_ridge = Ridge_predict(X_train, y_train_, X_test,y_test, 101.5646332755)\n",
    "y_pred_en, y_fitted_en, selected_features_en = EN_predict(X_train, y_train_, X_test,y_test, 0.5, 0.000345324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccecda-757b-410a-8383-9955f23b81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data, y_pred_lasso, y_fitted_lasso, regularizor = 'Lasso')\n",
    "plot(data, y_pred_ridge, y_fitted_ridge, regularizor = 'Ridge')\n",
    "plot(data, y_pred_en, y_fitted_en, regularizor = 'Elastic Net')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c87053-e220-4f7b-8dc4-ca0de8128e77",
   "metadata": {},
   "source": [
    "### Feature selection plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97103689-f962-4787-9b5e-9a7e5a99fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_combined = pd.concat([selected_features_lasso, selected_features_ridge,selected_features_en], axis=1)\n",
    "selected_features_combined = selected_features_combined[(selected_features_combined != 0).any(axis=1)]\n",
    "selected_features_combined.columns = ['Lasso','Ridge','Elastic Net']\n",
    "cleaned_index = [label.replace(\"jkp_factor_\", \"\") for label in selected_features_combined.index]\n",
    "selected_features_combined.index = cleaned_index\n",
    "plt.figure(figsize=(10, 3))  # Change figsize to adjust for the transposed plot\n",
    "plt.imshow(selected_features_combined.T, cmap='binary', aspect='auto')  # Transpose selected_features_combined\n",
    "# Customize plot\n",
    "plt.yticks(ticks=range(len(selected_features_combined.columns)), labels=selected_features_combined.columns, rotation=45, fontsize=8)  # Change yticks to xticks\n",
    "plt.xticks(ticks=range(len(selected_features_combined.index)), labels=selected_features_combined.index, rotation=90, fontsize=8)  # Change xticks to yticks\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
