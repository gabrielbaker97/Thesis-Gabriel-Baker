{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b51fdf-4f08-4ef7-95f4-d891571d159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requirements.txt\", \"r\") as config_file:\n",
    "    config_code = config_file.read()\n",
    "    exec(config_code)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c18457-70c3-42c9-a88e-3580d8f519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_finance = sqlite3.connect(database=\"data/specialedata.sqlite\")\n",
    "\n",
    "macro_predictors = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM macro_predictors\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"macro_\")\n",
    ")\n",
    "\n",
    "JKPFactors = (pd.read_sql_query(\n",
    "  sql=\"SELECT * FROM JKPFactors\",\n",
    "  con=tidy_finance,\n",
    "  parse_dates={\"month\"})\n",
    "  .add_prefix(\"jkp_factor_\")\n",
    ")\n",
    "JKPFactornames = JKPFactors.columns\n",
    "\n",
    "factors_ff3_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff3_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff3_\")\n",
    ")\n",
    "\n",
    "factors_ff5_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff5_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff5_\")\n",
    ")\n",
    "\n",
    "ff_carhart = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM ff_carhart\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"ff_carhart_\")\n",
    ")\n",
    "crsp_2000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_2000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_250 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_250\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_100 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_100\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_50 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_50\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "# Select amount of tickers in cross section!\n",
    "data_total = (crsp_2000\n",
    "        .merge(factors_ff5_monthly,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"factor_ff5_month\")\n",
    "        .assign(ret_excess=lambda x: x[\"ret\"] - x[\"factor_ff5_rf\"]) \n",
    "        .drop(columns=['ret', 'factor_ff5_month'])\n",
    "        .dropna()\n",
    "       )\n",
    "\n",
    "# Make a dataframe for stock characteristics and factors\n",
    "macro_variables = data_total.filter(like=\"macro\").columns\n",
    "factor_variables = data_total.filter(like=\"jkp_factor\").columns\n",
    "macro_factors = data_total[macro_variables]\n",
    "factors = data_total[macro_variables].merge(data_total[factor_variables], left_index=True, right_index=True)\n",
    "char = data_total[['mktcap', 'mktcap_lag_1', 'mktcap_lag_3', 'mktcap_lag_6', 'mktcap_lag_12', 'mom_1', 'mom_3','mom_6', 'mom_12']]\n",
    "# List of tickers\n",
    "tickers = data_total['ticker'].unique()\n",
    "\n",
    "# Transform data\n",
    "column_combinations = list(product(macro_factors, char)) \n",
    "\n",
    "new_column_values = []\n",
    "for macro_column, char in column_combinations:\n",
    "    new_column_values.append(data_total[macro_column] * data_total[char])\n",
    "\n",
    "column_names = [\" x \".join(t) for t in column_combinations]\n",
    "new_columns = pd.DataFrame(dict(zip(column_names, new_column_values)))\n",
    "\n",
    "# New data set with added combinations\n",
    "data = pd.concat([data_total, new_columns], axis=1)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "  transformers=[\n",
    "    (\"scale\", StandardScaler(), \n",
    "    [col for col in data.columns \n",
    "      if col not in [\"ret_excess\", \"month\", \"ticker\"]])\n",
    "  ],\n",
    "  remainder=\"drop\",\n",
    "  verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "training_date = \"2017-07-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c890c-9f16-43b3-bcda-6772c8f87d80",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ce43c-0eea-4f0b-afbf-692f1ca1396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c4f1a-1c8d-484b-be0a-d771f7821673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF 5 factor model\n",
    "data_ff5 = data[['month','ticker','ret_excess','factor_ff5_mkt_excess','factor_ff5_smb','factor_ff5_hml','factor_ff5_rmw','factor_ff5_cma']]\n",
    "colnames = ['month','ticker','ret_excess','mkt_excess','smb','hml','rmw','cma']\n",
    "data_ff5.columns=colnames\n",
    "# FF 3 factor model\n",
    "data_ff3 = data_ff5.iloc[:,:6]\n",
    "# ff3\n",
    "data_capm = data_ff3.iloc[:,:4]\n",
    "\n",
    "data_carhart = (data_ff3\n",
    "        .merge(ff_carhart,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"ff_carhart_month\")\n",
    "        .drop(columns=['ff_carhart_mkt_excess','ff_carhart_month','ff_carhart_smb','ff_carhart_hml', 'ff_carhart_rf'])\n",
    "        .dropna()\n",
    "       )\n",
    "data_carhart.columns = ['month','ticker','ret_excess','mkt_excess','smb','hml','mom']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02280737-f096-44ae-a2dd-3fc03b6d0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CAPM Factor Model \n",
    "df_capm = data_capm.copy()\n",
    "df_capm.set_index('month', inplace=True)\n",
    "y_capm = df_capm['ret_excess']\n",
    "X_capm = df_capm.drop(columns=['ret_excess', 'ticker'])\n",
    "\n",
    "# For FF 3 Factor Model \n",
    "df_ff3 = data_ff3.copy()\n",
    "df_ff3.set_index('month', inplace=True)\n",
    "y_ff3 = df_ff3['ret_excess']\n",
    "X_ff3 = df_ff3.drop(columns=['ret_excess', 'ticker'])\n",
    "\n",
    "# For FF 5 Factor Model \n",
    "df_ff5 = data_ff5.copy()\n",
    "df_ff5.set_index('month', inplace=True)\n",
    "y_ff5 = df_ff5['ret_excess']\n",
    "X_ff5 = df_ff5.drop(columns=['ret_excess', 'ticker'])\n",
    "\n",
    "# For Carhart Model \n",
    "df_carhart = data_carhart.copy()\n",
    "df_carhart.set_index('month', inplace=True)\n",
    "y_carhart = df_ff5['ret_excess']\n",
    "X_carhart = df_ff5.drop(columns=['ret_excess', 'ticker'])\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    training_date = \"2017-07-01\"\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_array = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled_array, index=X.index, columns=X.columns)\n",
    "    X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "    # Splitting data into training and testing sets\n",
    "    X_train = X_poly[X.index < training_date]\n",
    "    X_test = X_poly[X.index >= training_date]\n",
    "    y_train = y[y.index < training_date]\n",
    "    y_test = y[y.index >= training_date]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "\n",
    "X_train_capm, X_test_capm, y_train_capm, y_test_capm = preprocess_data(X_capm, y_capm)\n",
    "X_train_ff3, X_test_ff3, y_train_ff3, y_test_ff3 = preprocess_data(X_ff3, y_ff3)\n",
    "X_train_ff5, X_test_ff5, y_train_ff5, y_test_ff5 = preprocess_data(X_ff5, y_ff5)\n",
    "X_train_carhart, X_test_carhart, y_train_carhart, y_test_carhart = preprocess_data(X_carhart, y_carhart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390cfe03-e8dc-42e8-abe6-a7f9dc59f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capm\n",
    "num_estimators = [100, 150, 200]\n",
    "# Create an empty dictionary to store the models\n",
    "models_capm = {}\n",
    "predictions_capm = {}\n",
    "\n",
    "# Loop through the numbers of estimators\n",
    "for n in num_estimators:\n",
    "    start = time.time()\n",
    "\n",
    "    # Define and train the Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train_capm, y_train_capm)\n",
    "    end = time.time()\n",
    "    print(f'model_{n}: {round((end - start)/60,3)} minutes')\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    models_capm[f'model_{n}'] = model\n",
    "print('------------------')\n",
    "evaluation_results = {}\n",
    "\n",
    "# Loop through the models\n",
    "for model_name, model in models_capm.items():\n",
    "    # Evaluate the model\n",
    "    y_pred_capm = model.predict(X_test_capm)\n",
    "    mse = mean_squared_error(y_test_capm, y_pred_capm)\n",
    "    r2_capm = r2_score(y_test_capm, y_pred_capm)\n",
    "    \n",
    "    # Store the evaluation results in the dictionary\n",
    "    evaluation_results[model_name] = {'Mean Squared Error': mse, 'R^2 Score': r2_capm}\n",
    "# Print the evaluation results\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"capm Evaluation results for {model_name}:\")\n",
    "    print(\"Mean Squared Error:\", round(results['Mean Squared Error'],4))\n",
    "    print(\"R^2 Score:\", round(results['R^2 Score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef91acd-53de-4d52-9113-5ba2cb04ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff3\n",
    "num_estimators = [100, 150, 200]\n",
    "\n",
    "# Create an empty dictionary to store the models\n",
    "models_ff3 = {}\n",
    "predictions_ff3 = {}\n",
    "\n",
    "# Loop through the numbers of estimators\n",
    "for n in num_estimators:\n",
    "    start = time.time()\n",
    "\n",
    "    # Define and train the Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train_ff3, y_train_ff3)\n",
    "    end = time.time()\n",
    "    print(f'model_{n}: {round((end - start)/60,3)} minutes')\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    models_ff3[f'model_{n}'] = model\n",
    "print('------------------')\n",
    "evaluation_results = {}\n",
    "\n",
    "# Loop through the models\n",
    "for model_name, model in models_ff3.items():\n",
    "    # Evaluate the model\n",
    "    y_pred_ff3 = model.predict(X_test_ff3)\n",
    "    mse = mean_squared_error(y_test_ff3, y_pred_ff3)\n",
    "    r2_ff3 = r2_score(y_test_ff3, y_pred_ff3)\n",
    "    \n",
    "    # Store the evaluation results in the dictionary\n",
    "    evaluation_results[model_name] = {'Mean Squared Error': mse, 'R^2 Score': r2_ff3}\n",
    "# Print the evaluation results\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"ff3 Evaluation results for {model_name}:\")\n",
    "    print(\"R^2 Score:\", round(results['R^2 Score'],4))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cb878-2721-4f9e-b2a4-7de0417b88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carhart\n",
    "num_estimators =  [100, 150, 200]\n",
    "\n",
    "# Create an empty dictionary to store the models\n",
    "models_carhart = {}\n",
    "predictions_carhart = {}\n",
    "\n",
    "# Loop through the numbers of estimators\n",
    "for n in num_estimators:\n",
    "    start = time.time()\n",
    "\n",
    "    # Define and train the Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train_carhart, y_train_carhart)\n",
    "    end = time.time()\n",
    "    print(f'model_{n}: {round((end - start)/60,3)} minutes')\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    models_carhart[f'model_{n}'] = model\n",
    "print('------------------')\n",
    "evaluation_results = {}\n",
    "\n",
    "# Loop through the models\n",
    "for model_name, model in models_carhart.items():\n",
    "    # Evaluate the model\n",
    "    y_pred_carhart = model.predict(X_test_carhart)\n",
    "    mse = mean_squared_error(y_test_carhart, y_pred_carhart)\n",
    "    r2_carhart = r2_score(y_test_carhart, y_pred_carhart)\n",
    "    \n",
    "    # Store the evaluation results in the dictionary\n",
    "    evaluation_results[model_name] = {'Mean Squared Error': mse, 'R^2 Score': r2_carhart}\n",
    "# Print the evaluation results\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"carhart Evaluation results for {model_name}:\")\n",
    "    print(\"Mean Squared Error:\", round(results['Mean Squared Error'],4))\n",
    "    print(\"R^2 Score:\", round(results['R^2 Score'],4))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff23745-2638-42b5-a921-d44a611fb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff5\n",
    "num_estimators =  [100, 150, 200]\n",
    "\n",
    "# Create an empty dictionary to store the models\n",
    "models_ff5 = {}\n",
    "predictions_ff5 = {}\n",
    "\n",
    "# Loop through the numbers of estimators\n",
    "for n in num_estimators:\n",
    "    start = time.time()\n",
    "\n",
    "    # Define and train the Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train_ff5, y_train_ff5)\n",
    "    end = time.time()\n",
    "    print(f'model_{n}: {round((end - start)/60,3)} minutes')\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    models_ff5[f'model_{n}'] = model\n",
    "print('------------------')\n",
    "evaluation_results = {}\n",
    "\n",
    "# Loop through the models\n",
    "for model_name, model in models_ff5.items():\n",
    "    # Evaluate the model\n",
    "    y_pred_ff5 = model.predict(X_test_ff5)\n",
    "    mse = mean_squared_error(y_test_ff5, y_pred_ff5)\n",
    "    r2_ff5 = r2_score(y_test_ff5, y_pred_ff5)\n",
    "    \n",
    "    # Store the evaluation results in the dictionary\n",
    "    evaluation_results[model_name] = {'Mean Squared Error': mse, 'R^2 Score': r2_ff5}\n",
    "# Print the evaluation results\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"ff5 Evaluation results for {model_name}:\")\n",
    "    print(\"Mean Squared Error:\", round(results['Mean Squared Error'],4))\n",
    "    print(\"R^2 Score:\", round(results['R^2 Score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dc5ff-9a6c-4866-ba17-02eca3e2797c",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3d5da-af76-4946-b94b-7fe142cc6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF 5 factor model\n",
    "data_ff5 = data[['month','ticker','ret_excess','factor_ff5_mkt_excess','factor_ff5_smb','factor_ff5_hml','factor_ff5_rmw','factor_ff5_cma']]\n",
    "colnames = ['month','ticker','ret_excess','mkt_excess','smb','hml','rmw','cma']\n",
    "data_ff5.columns=colnames\n",
    "# FF 3 factor model\n",
    "data_ff3 = data_ff5.iloc[:,:6]\n",
    "# ff3\n",
    "data_capm = data_ff3.iloc[:,:4]\n",
    "# FF 5 factor model\n",
    "data_ff5 = data[['month','ticker','ret_excess','factor_ff5_mkt_excess','factor_ff5_smb','factor_ff5_hml','factor_ff5_rmw','factor_ff5_cma']]\n",
    "colnames = ['month','ticker','ret_excess','mkt_excess','smb','hml','rmw','cma']\n",
    "data_ff5.columns=colnames\n",
    "# FF 3 factor model\n",
    "data_ff3 = data_ff5.iloc[:,:6]\n",
    "# ff3\n",
    "data_capm = data_ff3.iloc[:,:4]\n",
    "\n",
    "# For FF 5 Factor Model (data_ff5)\n",
    "df_capm = data_capm.copy()\n",
    "df_capm.set_index('month', inplace=True)\n",
    "y_capm = df_capm['ret_excess']\n",
    "X_capm = df_capm.drop(columns=['ret_excess', 'ticker'])\n",
    "\n",
    "# For FF 3 Factor Model (data_ff3)\n",
    "df_ff3 = data_ff3.copy()\n",
    "df_ff3.set_index('month', inplace=True)\n",
    "y_ff3 = df_ff3['ret_excess']\n",
    "X_ff3 = df_ff3.drop(columns=['ret_excess', 'ticker'])\n",
    "\n",
    "# For ff3 (data_ff3)\n",
    "df_ff5 = data_ff3.copy()\n",
    "df_ff5.set_index('month', inplace=True)\n",
    "y_ff5 = df_ff3['ret_excess']\n",
    "X_ff5 = df_ff3.drop(columns=['ret_excess', 'ticker'])\n",
    "X_train_capm, X_test_capm, y_train_capm, y_test_capm = preprocess_data(X_capm, y_capm)\n",
    "X_train_ff3, X_test_ff3, y_train_ff3, y_test_ff3 = preprocess_data(X_ff3, y_ff3)\n",
    "X_train_ff5, X_test_ff5, y_train_ff5, y_test_ff5 = preprocess_data(X_ff5, y_ff5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912960ce-0185-4a31-b9d4-c485612d65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(layers, neurons, n_features):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(neurons[0], activation='sigmoid', input_shape=(n_features,), name='input_layer'))\n",
    "    \n",
    "    for i in range(layers):\n",
    "        model.add(keras.layers.Dense(neurons[i], activation='sigmoid', name=f'hidden_layer_{i+1}'))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1, name='output_layer'))  # Output layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08777a7f-7cc1-4f5a-932d-a07fd36a3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_capm = X_train_capm.shape[1]\n",
    "n_features_ff3 = X_train_ff3.shape[1]\n",
    "n_features_ff5 = X_train_ff5.shape[1]\n",
    "n_features_carhart = X_train_carhart.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35755640-39d4-43c6-88a1-5d61304e8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAPM\n",
    "model_4_capm = create_neural_network(4, [2,2,2,1], n_features_capm)\n",
    "model_4_capm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_5_capm = create_neural_network(5, [2,2,2,2,1], n_features_capm)\n",
    "model_5_capm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_6_capm = create_neural_network(5, [2,2,2,2,2,1], n_features_capm)\n",
    "model_6_capm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#FF3\n",
    "model_4_ff3 = create_neural_network(4, [2,2,2,1], n_features_ff3)\n",
    "model_4_ff3.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_5_ff3 = create_neural_network(5, [2,2,2,2,1], n_features_ff3)\n",
    "model_5_ff3.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_6_ff3 = create_neural_network(5, [2,2,2,2,2,1], n_features_ff3)\n",
    "model_6_ff3.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#FF5\n",
    "model_4_ff5 = create_neural_network(4, [2,2,2,1], n_features_ff5)\n",
    "model_4_ff5.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_5_ff5 = create_neural_network(5, [4,2,2,2,1], n_features_ff5)\n",
    "model_5_ff5.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_6_ff5 = create_neural_network(5, [4,2,2,2,2,1], n_features_ff5)\n",
    "model_6_ff5.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#Carhart\n",
    "model_4_carhart = create_neural_network(4, [2,2,2,1], n_features_carhart)\n",
    "model_4_carhart.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_5_carhart = create_neural_network(5, [2,2,2,2,1], n_features_carhart)\n",
    "model_5_carhart.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_6_carhart = create_neural_network(5, [2,2,2,2,2,1], n_features_carhart)\n",
    "model_6_carhart.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b428038-8317-4f55-8d90-c7e26ad1d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(model, X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=50, verbose = 0,  batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R^2 score and MSPE\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mspe = mean_squared_error(y_test, y_pred)\n",
    "    print(\" Out-of-sample R^2:\", round(r2,4))\n",
    "    #print(\"Mean Squared Prediction Error (MSPE):\", round(mspe,4))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'{round(end - start,3)/60} minutes')\n",
    "models_capm = [model_4_capm,model_5_capm,model_6_capm]\n",
    "models_ff3 = [model_4_ff3,model_5_ff3,model_6_ff3]\n",
    "models_ff5 = [model_4_ff5,model_5_ff5,model_6_ff5]\n",
    "models_carhart = [model_4_carhart,model_5_carhart,model_6_carhart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422948a-7f8e-4384-be98-09cbc39beac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capm\n",
    "for model in models_capm:\n",
    "    NN(model, X_train_capm, y_train_capm, X_test_capm, y_test_capm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4b5f3-11b8-495d-86f4-7ce4ddff9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF3\n",
    "for model in models_ff3:\n",
    "    NN(model, X_train_ff3, y_train_ff3, X_test_ff3, y_test_ff3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf0b34-b8c3-4a5f-8458-a16c0ef43cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carhart\n",
    "for model in models_carhart:\n",
    "    NN(model, X_train_carhart, y_train_carhart, X_test_carhart, y_test_carhart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e6ce7-4bea-4bd5-97bd-089b51497073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff5\n",
    "for model in models_ff5:\n",
    "    NN(model, X_train_ff5, y_train_ff5, X_test_ff5, y_test_ff5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
