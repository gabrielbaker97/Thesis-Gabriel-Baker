{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a49d6b-d488-4fdb-a73a-5a8ec710fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pyreadr\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import plots as pl\n",
    "import getFamaFrenchFactors as gff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6c7cc-3ba4-4e62-85b4-97ed01f05c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2000-01-01')\n",
    "end_date = pd.to_datetime('2022-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2f238-d572-4da0-b7f1-a632fef398e5",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbf04b-2612-4f37-9f18-ba6df9ec4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_finance = sqlite3.connect(database=\"data/specialedata.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf541987-4230-4d2b-b492-889ee98146a5",
   "metadata": {},
   "source": [
    "### Macroeconomic predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddac75d-0c6f-4fdf-81fe-0cc3cfd019ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sheet_id = \"1g4LOaRj4TvwJr9RIaA_nwrXXWTOy46bP\"\n",
    "sheet_name = \"macro_predictors.xlsx\"\n",
    "macro_predictors_link = (\n",
    "  f\"https://docs.google.com/spreadsheets/d/{sheet_id}\" \n",
    "  f\"/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    ")\n",
    "\n",
    "macro_predictors = (\n",
    "  pd.read_csv(macro_predictors_link, thousands=\",\")\n",
    "  .assign(\n",
    "    month=lambda x: pd.to_datetime(x[\"yyyymm\"], format=\"%Y%m\") + pd.offsets.MonthEnd(0),\n",
    "    dp=lambda x: np.log(x[\"D12\"])-np.log(x[\"Index\"]),\n",
    "    dy=lambda x: np.log(x[\"D12\"])-np.log(x[\"D12\"].shift(1)),\n",
    "    ep=lambda x: np.log(x[\"E12\"])-np.log(x[\"Index\"]),\n",
    "    de=lambda x: np.log(x[\"D12\"])-np.log(x[\"E12\"]),\n",
    "    tms=lambda x: x[\"lty\"]-x[\"tbl\"],\n",
    "    dfy=lambda x: x[\"BAA\"]-x[\"AAA\"]\n",
    "  )\n",
    "  .rename(columns={\"b/m\": \"bm\"})\n",
    "  .get([\"month\", \"dp\", \"dy\", \"ep\", \"de\", \"svar\", \"bm\", \n",
    "        \"ntis\", \"tbl\", \"lty\", \"ltr\", \"tms\", \"dfy\", \"infl\"])\n",
    "  .query(\"month >= @start_date and month <= @end_date\")\n",
    "  .dropna()\n",
    ")\n",
    "macro_predictors['month'] = pd.to_datetime(macro_predictors['month']).dt.to_period('M')\n",
    "macro_predictors['month'] = macro_predictors['month'].dt.to_timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082da3ee-5ef8-46b1-a222-84a491a2513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_keys=macro_predictors.columns.tolist()[1:]\n",
    "macro_value = ['Dividend- Price Ratio',\n",
    "              'Dividend Yield',\n",
    "              'Earnings Price Ratio',\n",
    "              'Dividend Payout Ratio',\n",
    "              'Stock Variance',\n",
    "              'Book-to-Market Ratio',\n",
    "              'Net Equity Expansion',\n",
    "              'Treasury Bills (3 month)',\n",
    "              'Long-Term Yield',\n",
    "              'Long-Term Rate of Return',\n",
    "              'Term Spread',\n",
    "              'Default Yield Spread', 'Inflation']\n",
    "macro_value1 = ['Month', 'Dividend- Price Ratio',\n",
    " 'Dividend Yield',\n",
    " 'Earnings Price Ratio',\n",
    " 'Dividend Payout Ratio',\n",
    " 'Stock Variance',\n",
    " 'Book-to-Market Ratio',\n",
    " 'Net Equity Expansion',\n",
    " 'Treasury Bills (3 month)',\n",
    " 'Long-Term Yield',\n",
    " 'Long-Term Rate of Return',\n",
    " 'Term Spread',\n",
    " 'Default Yield Spread',\n",
    " 'Inflation']\n",
    "macro_description = ['The dividend price ratio (dp), the difference between the log of dividends and the log of prices, where dividends are 12-month moving sums of dividends paid on the S&P 500 index, and prices are monthly averages of daily closing prices (',\n",
    "                'Dividend yield (dy), the difference between the log of dividends and the log of lagged prices (Ball 1978).',\n",
    "               'Earnings price ratio (ep), the difference between the log of earnings and the log of prices, where earnings are 12-month moving sums of earnings on the S&P 500 index',\n",
    "               'Dividend payout ratio (de), the difference between the log of dividends and the log of earnings',\n",
    "               'Stock variance (svar), the sum of squared daily returns on the S&P 500 index',\n",
    "               'Book-to-market ratio (bm), the ratio of book value to market value for the Dow Jones Industrial Average',\n",
    "               'Net equity expansion (ntis), the ratio of 12-month moving sums of net issues by NYSE listed stocks divided by the total end-of-year market capitalization of NYSE stocks',\n",
    "               'Treasury bills (tbl), the 3-Month Treasury Bill: Secondary Market Rate from the economic research database at the Federal Reserve Bank at St. Louis',\n",
    "               'Long-term yield (lty), the long-term government bond yield from Ibbotsonâ€™s Stocks, Bonds, Bills, and Inflation Yearbook',\n",
    "               'Long-term rate of returns (ltr), the long-term government bond returns from Ibbotsonâ€™s Stocks, Bonds, Bills, and Inflation Yearbook ',\n",
    "               'Term spread (tms), the difference between the long-term yield on government bonds and the Treasury bill',\n",
    "               'Default yield spread (dfy), the difference between BAA and AAA-rated corporate bond yields',\n",
    "               'Inflation (infl), the Consumer Price Index (All Urban Consumers) from the Bureau of Labor Statistics']\n",
    "description_macro_dict = dict(zip(macro_value, macro_description))\n",
    "macros = dict(zip(macro_keys, description_macro_dict))\n",
    "macros = {key: (value, description) for key, value, description in zip(macro_keys, macro_value, macro_description)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65940a-c504-4d95-9983-160dd09d986a",
   "metadata": {},
   "source": [
    "### Fama-French Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5035ca7d-6dfe-46b6-92e8-9f747c71b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_ff3_monthly_raw = pdr.DataReader(\n",
    "  name=\"F-F_Research_Data_Factors\",\n",
    "  data_source=\"famafrench\", \n",
    "  start=start_date, \n",
    "  end=end_date)[0]\n",
    "\n",
    "factors_ff3_monthly = (factors_ff3_monthly_raw\n",
    "  .divide(100)\n",
    "  .reset_index(names=\"month\")\n",
    "  .assign(month=lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n",
    "  .rename(str.lower, axis=\"columns\")\n",
    "  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n",
    ")\n",
    "\n",
    "factors_ff5_monthly_raw = pdr.DataReader(\n",
    "  name=\"F-F_Research_Data_5_Factors_2x3\",\n",
    "  data_source=\"famafrench\", \n",
    "  start=start_date, \n",
    "  end=end_date)[0]\n",
    "\n",
    "factors_ff5_monthly = (factors_ff5_monthly_raw\n",
    "  .divide(100)\n",
    "  .reset_index(names=\"month\")\n",
    "  .assign(month=lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n",
    "  .rename(str.lower, axis=\"columns\")\n",
    "  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n",
    ")\n",
    "\n",
    "carhart = gff.carhart4Factor(frequency='m')[['date_ff_factors','MOM']]\n",
    "mom = carhart[(carhart['date_ff_factors'] >= start_date) & (carhart['date_ff_factors'] <= end_date)]\n",
    "mom.columns = ['month','mom']\n",
    "mom['month'] = mom['month'] + pd.offsets.MonthBegin(0)\n",
    "factors_ff3_monthly = (factors_ff3_monthly_raw\n",
    "  .divide(100)\n",
    "  .reset_index(names=\"month\")\n",
    "  .assign(month=lambda x: pd.to_datetime(x[\"month\"].astype(str)))\n",
    "  .rename(str.lower, axis=\"columns\")\n",
    "  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n",
    ")\n",
    "ff_carhart = pd.merge(mom, factors_ff3_monthly, on=\"month\", how=\"inner\")\n",
    "\n",
    "\n",
    "factors_ff3_monthly['month'] = pd.to_datetime(factors_ff3_monthly['month']).dt.to_period('M')\n",
    "factors_ff5_monthly['month'] = pd.to_datetime(factors_ff5_monthly['month']).dt.to_period('M')\n",
    "factors_ff3_monthly['month'] = factors_ff3_monthly['month'].dt.to_timestamp()\n",
    "factors_ff5_monthly['month'] = factors_ff5_monthly['month'].dt.to_timestamp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9ae24-54b4-4b73-b3f0-c04f53ee6332",
   "metadata": {},
   "source": [
    "### JKP Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c58204-dc1a-46f4-a828-2f4aeda5dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download JKPFactors JKP\n",
    "JKP = pd.read_csv(\"factors.csv\").drop([\"location\", \"freq\", \"weighting\", \"direction\", \"n_stocks\", \"n_stocks_min\"], axis = 1) # Remove redundant columns\n",
    "#JKP = JKP.drop([\"location\", \"freq\", \"weighting\", \"direction\", \"n_stocks\", \"n_stocks_min\"], axis = 1) # Remove redundant columns\n",
    "JKP.rename(columns = {'date': 'month'}, inplace = True) # Rename date column\n",
    "JKP = JKP[['month', 'name', 'ret']] # select column order\n",
    "JKP['month'] = pd.to_datetime(JKP['month']) # select relevant period\n",
    "JKP = JKP[(JKP['month'] >= start_date) & (JKP['month'] <= end_date)]\n",
    "JKPFactors = JKP.pivot(index='month', columns='name', values='ret')\n",
    "JKPFactors.reset_index(inplace=True)\n",
    "JKPFactornames = JKPFactors.columns\n",
    "JKPFactors['month'] = pd.to_datetime(JKPFactors['month']).dt.to_period('M')\n",
    "JKPFactors['month'] = JKPFactors['month'].dt.to_timestamp()\n",
    "print(f\"The number of periods is {JKPFactors.shape[0]} and the number of predictors is {JKPFactors.shape[1]-1}\")\n",
    "# JKPFactors # Print factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf509768-ac55-4d49-8125-dd88d30acbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_counts(dataframe):\n",
    "    nan_counts = dataframe.isna().sum()\n",
    "    nonzero = nan_counts[nan_counts != 0]\n",
    "    \n",
    "    if not nonzero.empty:\n",
    "        print(\"Columns with non-zero NaN counts:\")\n",
    "        print(nonzero)\n",
    "    else:\n",
    "        print(\"No NaN values found in any column.\")\n",
    "#check_nan_counts(JKPFactors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb335413-f3a6-452d-8ac6-6b4e226ffdd5",
   "metadata": {},
   "source": [
    "# CRSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26262677-c781-4ac0-bc83-5101363813f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features\n",
    "sfz_mth = pyreadr.read_r('crsp/siz202212_r/sfz_mth.rds')\n",
    "df = sfz_mth[None] # extract the pandas data frame \n",
    "df= df[['KYPERMNO', 'MCALDT', 'MRET', 'MTCAP', 'MVOL']]\n",
    "colnames = ['permno', 'month', 'ret', 'mktcap', 'volume']\n",
    "df.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8addd20a-3fe4-47f1-8492-5cadcd067ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect industry data\n",
    "sfz_nam = pyreadr.read_r('crsp/siz202212_r/sfz_nam.rds')\n",
    "exchanges = sfz_nam[None]['EXCHCD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901d480-39ce-40e9-afe2-5a5f426fb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfz_nam = pyreadr.read_r('crsp/siz202212_r/sfz_nam.rds')\n",
    "headers = sfz_nam[None][['KYPERMNO', 'TICKER', 'SICCD']] # extract the pandas data frame \n",
    "hdr_colnames = ['permno', 'ticker', 'sic']\n",
    "headers.columns = hdr_colnames\n",
    "headers = headers.drop_duplicates(subset=['permno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a424b1-4534-4025-ac1b-1b3d893ca3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe based on the specified date range\n",
    "df['month'] = pd.to_datetime(df['month'])\n",
    "filtered_df = pd.DataFrame(df[(df['month'] >= start_date) & (df['month'] <= end_date)])\n",
    "# Rename columns\n",
    "df_merged = pd.merge(filtered_df, headers, on ='permno', how = 'left').dropna()\n",
    "df_merged['month'] = pd.to_datetime(df_merged['month']).dt.date\n",
    "df_merged = pd.DataFrame(df_merged[['month','ticker', 'sic','ret','mktcap','volume']])\n",
    "df_merged['month'] = pd.to_datetime(df_merged['month']).dt.to_period('M')\n",
    "df_merged['month'] = df_merged['month'].dt.to_timestamp()\n",
    "\n",
    "# Select tickers where data is available for the entire period\n",
    "total_dates = len(pd.date_range(start=start_date, end=end_date, freq='M'))\n",
    "\n",
    "# Group by ticker and check if the count of unique dates is equal to total_dates\n",
    "valid_tickers = df_merged.groupby('ticker')['month'].nunique() == total_dates\n",
    "valid_tickers_count = (df_merged.groupby('ticker')['month'].nunique() == total_dates).sum()\n",
    "valid_tickers_count # 2017 valid tickers\n",
    "\n",
    "# Filter for valid tickers\n",
    "df_filtered1 = df_merged[df_merged['ticker'].isin(valid_tickers[valid_tickers].index)]\n",
    "\n",
    "# Format market capitalization to millions\n",
    "df_filtered1['mktcap'] = df_filtered1['mktcap']/1000000\n",
    "df_filtered1['volume'] = df_filtered1['volume']/1000000\n",
    "\n",
    "avg_mktcap = df_filtered1.groupby('ticker')['mktcap'].mean().reset_index()\n",
    "top_50_tickers = list(avg_mktcap.nlargest(50,'mktcap')['ticker'])\n",
    "\n",
    "# Select largest companies\n",
    "df_filtered2 = df_filtered1[df_filtered1['ticker'].isin(top_50_tickers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2489d-8101-4b5a-bb4b-a5dc32e755b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find largest companies on average in the period\n",
    "top_2000_tickers = list(avg_mktcap.nlargest(2000,'mktcap')['ticker'])\n",
    "top_1500_tickers = list(avg_mktcap.nlargest(1500,'mktcap')['ticker'])\n",
    "top_1000_tickers = list(avg_mktcap.nlargest(1000,'mktcap')['ticker'])\n",
    "top_500_tickers = list(avg_mktcap.nlargest(500,'mktcap')['ticker'])\n",
    "top_250_tickers = list(avg_mktcap.nlargest(250,'mktcap')['ticker'])\n",
    "top_100_tickers = list(avg_mktcap.nlargest(100,'mktcap')['ticker'])\n",
    "top_50_tickers = list(avg_mktcap.nlargest(50,'mktcap')['ticker'])\n",
    "\n",
    "# Dataframes\n",
    "df_2000 = df_filtered1[df_filtered1['ticker'].isin(top_2000_tickers)]\n",
    "df_1500 = df_filtered1[df_filtered1['ticker'].isin(top_1500_tickers)]\n",
    "df_1000 = df_filtered1[df_filtered1['ticker'].isin(top_1000_tickers)]\n",
    "df_500 = df_filtered1[df_filtered1['ticker'].isin(top_500_tickers)]\n",
    "df_250 = df_filtered1[df_filtered1['ticker'].isin(top_250_tickers)]\n",
    "df_100 = df_filtered1[df_filtered1['ticker'].isin(top_100_tickers)]\n",
    "df_50 = df_filtered1[df_filtered1['ticker'].isin(top_50_tickers)]\n",
    "\n",
    "crsp_2000 = df_2000.groupby(['ticker', 'month']).agg({'sic': 'first',  'ret': 'mean', 'mktcap': 'mean', 'volume':'mean'}).reset_index()\n",
    "crsp_1500 = df_1500.groupby(['ticker', 'month']).agg({'sic': 'first',  'ret': 'mean', 'mktcap': 'mean', 'volume':'mean'}).reset_index()\n",
    "crsp_1000 = df_1000.groupby(['ticker', 'month']).agg({'sic': 'first',  'ret': 'mean', 'mktcap': 'mean', 'volume':'mean'}).reset_index()\n",
    "crsp_500 = df_500.groupby(['ticker', 'month']).agg({'sic': 'first',  'ret': 'mean', 'mktcap': 'mean', 'volume':'mean'}).reset_index()\n",
    "crsp_250 = df_250.groupby(['ticker', 'month']).agg({'sic': 'first',  'ret': 'mean', 'mktcap': 'mean', 'volume':'mean'}).reset_index()\n",
    "crsp_100 = df_100.groupby(['ticker', 'month']).agg({'sic': 'first',  'ret': 'mean', 'mktcap': 'mean', 'volume':'mean'}).reset_index()\n",
    "crsp_50 = df_50.groupby(['ticker', 'month']).agg({'sic': 'first',  'ret': 'mean', 'mktcap': 'mean', 'volume':'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9232e76-becd-4e3c-8c07-9d272932bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ticker and month, and aggregate using mean \n",
    "crsp = df_filtered2.groupby(['ticker', 'month']).agg({\n",
    "    'sic': 'first',  \n",
    "    'ret': 'mean',   \n",
    "    'mktcap': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Print the updated DataFrame\n",
    "#print('Number of stocks', crsp['ticker'].unique().shape[0], '\\nObservations per ticker:', crsp['ticker'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9e166-1943-4d3e-b25c-e49f55c6c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_mktcap(df):\n",
    "    # Sort the dataframe by ticker and month\n",
    "    df.sort_values(['ticker', 'month'], inplace=True)\n",
    "    \n",
    "    # Create lagged marketcapitalization columns\n",
    "    df['mktcap_lag_1'] = df.groupby('ticker')['mktcap'].shift(-1)\n",
    "    df['mktcap_lag_3'] = df.groupby('ticker')['mktcap'].shift(-3)\n",
    "    df['mktcap_lag_6'] = df.groupby('ticker')['mktcap'].shift(-6)\n",
    "    df['mktcap_lag_12'] = df.groupby('ticker')['mktcap'].shift(-12)\n",
    "    \n",
    "    # Reset sorting order and drop rows with missing values\n",
    "    df.sort_index(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "# Apply lagged mktcap function to each dataframe\n",
    "add_lagged_mktcap(crsp_2000)\n",
    "add_lagged_mktcap(crsp_1500)\n",
    "add_lagged_mktcap(crsp_1000)\n",
    "add_lagged_mktcap(crsp_500)\n",
    "add_lagged_mktcap(crsp_250)\n",
    "add_lagged_mktcap(crsp_100)\n",
    "add_lagged_mktcap(crsp_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87f876-aa30-4844-b1b9-4f83f01bb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_momentum(df):\n",
    "    # Conditionally compute 'Mom_1'\n",
    "    condition_mom_1_3 = ((df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(years=1))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=1))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=2))))\n",
    "    df['mom_1'] = np.where(condition_mom_1_3,\n",
    "                           100 * (df['mktcap'] - df['mktcap_lag_1']) / df['mktcap_lag_1'],\n",
    "                           np.nan)\n",
    "\n",
    "    # Conditionally compute 'Mom_3'\n",
    "    condition_mom_1_3 = ((df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(years=1))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=1))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=2))))\n",
    "    df['mom_3'] = np.where(condition_mom_1_3,\n",
    "                           100 * (df['mktcap'] - df['mktcap_lag_3']) / df['mktcap_lag_3'],\n",
    "                           np.nan)\n",
    "\n",
    "    # Conditionally compute 'Mom_6'\n",
    "    condition_mom_1_6 = ((df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(years=1))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=1))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=2))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=3))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=4))) &\n",
    "                         (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=5))))\n",
    "    df['mom_6'] = np.where(condition_mom_1_6,\n",
    "                           100 * (df['mktcap'] - df['mktcap_lag_6']) / df['mktcap_lag_6'],\n",
    "                           np.nan)\n",
    "\n",
    "    # Conditionally compute 'Mom_12'\n",
    "    condition_mom_1_12 = ((df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(years=1))) &\n",
    "                          (df['month'] != (pd.to_datetime(df['month']) - pd.DateOffset(months=1))))\n",
    "    df['mom_12'] = np.where(condition_mom_1_12,\n",
    "                            100 * (df['mktcap_lag_1'] - df['mktcap_lag_12']) / df['mktcap_lag_12'],\n",
    "                            np.nan)\n",
    "\n",
    "# Apply mom function to each dataframe\n",
    "compute_momentum(crsp_2000)\n",
    "compute_momentum(crsp_1500)\n",
    "compute_momentum(crsp_1000)\n",
    "compute_momentum(crsp_500)\n",
    "compute_momentum(crsp_250)\n",
    "compute_momentum(crsp_100)\n",
    "compute_momentum(crsp_50)\n",
    "#compute_momentum(crsp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee11a0f-232e-4515-a8a3-0dcafaa5a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant order of columns and look at the dataframe\n",
    "selected_columns=['month', 'ticker', 'ret', 'mktcap', 'mktcap_lag_1', 'mktcap_lag_3', 'mktcap_lag_6', 'mktcap_lag_12', 'mom_1', 'mom_3', 'mom_6', 'mom_12', 'volume']\n",
    "#crsp = crsp[['month', 'ticker', 'ret', 'mktcap', 'mktcap_lag_1', 'mktcap_lag_3', 'mktcap_lag_6', 'mktcap_lag_12', 'mom_1', 'mom_3', 'mom_6', 'mom_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaa3b7-e32d-467c-8d3a-3ca1e2887e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_2000 = crsp_2000[selected_columns]\n",
    "crsp_1500 = crsp_1500[selected_columns]\n",
    "crsp_1000 = crsp_1000[selected_columns]\n",
    "crsp_500 = crsp_500[selected_columns]\n",
    "crsp_250 = crsp_250[selected_columns]\n",
    "crsp_100 = crsp_100[selected_columns]\n",
    "crsp_50 = crsp_50[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182038da-91d7-4d26-b148-fb253f203d9c",
   "metadata": {},
   "source": [
    "### Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bd647-62dd-4d57-a00c-5a83424eb642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"factors_ff3_monthly\": factors_ff3_monthly,\n",
    "    \"factors_ff5_monthly\": factors_ff5_monthly,\n",
    "    \"ff_carhart\": ff_carhart,\n",
    "    \"macro_predictors\": macro_predictors,\n",
    "    \"JKPFactors\": JKPFactors,\n",
    "    \"crsp_2000\": crsp_2000,\n",
    "    \"crsp_1500\": crsp_1500,\n",
    "    \"crsp_1000\": crsp_1000,\n",
    "    \"crsp_500\": crsp_500,\n",
    "    \"crsp_250\": crsp_250,\n",
    "    \"crsp_100\": crsp_100,\n",
    "    \"crsp_50\": crsp_50\n",
    "}\n",
    "for key, value in data_dict.items():\n",
    "    value.to_sql(name=key,\n",
    "                 con=tidy_finance, \n",
    "                 if_exists=\"replace\",\n",
    "                 index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5be9e-abbc-443a-a07c-4f0459b53384",
   "metadata": {},
   "source": [
    "# Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49f778-368e-4847-90fb-ca0387b9a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_predictors = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM macro_predictors\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"macro_\")\n",
    ")\n",
    "\n",
    "JKPFactors = (pd.read_sql_query(\n",
    "  sql=\"SELECT * FROM JKPFactors\",\n",
    "  con=tidy_finance,\n",
    "  parse_dates={\"month\"})\n",
    "  .add_prefix(\"jkp_factor_\")\n",
    ")\n",
    "JKPFactornames = JKPFactors.columns\n",
    "\n",
    "factors_ff3_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff3_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff3_\")\n",
    ")\n",
    "\n",
    "factors_ff5_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff5_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff5_\")\n",
    ")\n",
    "crsp_2000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_2000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_250 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_250\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_100 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_100\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_50 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_50\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf04a51-7104-44e6-a393-9e181474eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [crsp_50, crsp_100, crsp_250, crsp_500, crsp_1000, crsp_1500, crsp_2000]\n",
    "\n",
    "# Create an empty dictionary to store dataframes\n",
    "data_dict = {}\n",
    "\n",
    "# Loop through each dataset in the data_list\n",
    "for i, data in enumerate(data_list, start=50):\n",
    "    # Merge datasets and perform required operations\n",
    "    merged_data = (data\n",
    "                   .merge(JKPFactors,\n",
    "                          how=\"left\", left_on=\"month\", right_on=\"jkp_factor_month\")\n",
    "                   .merge(macro_predictors,\n",
    "                          how=\"left\", left_on=\"month\", right_on=\"macro_month\")\n",
    "                   .assign(ret_excess=lambda x: x[\"ret\"] - x[\"macro_lty\"])\n",
    "                   .drop(columns=['ret', 'jkp_factor_month', 'macro_month'])\n",
    "                   .dropna()\n",
    "                   )\n",
    "    # Assign the merged data to a dataframe variable\n",
    "    data_dict[f\"data_{i}\"] = merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f510d23-9dcb-4061-90a6-422e72b3be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(crsp_2000['ticker'].unique())\n",
    "# Specify the file path where you want to save the text file\n",
    "file_path = 'tickers.txt'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, 'w') as file:\n",
    "    # Write each ticker name to the file\n",
    "    for ticker in tickers:\n",
    "        file.write(ticker + ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0d4be-2b6c-44b6-8950-a2533551cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives_dict = {}\n",
    "\n",
    "# Loop through each dataset in data_dict\n",
    "for key, data_df in data_dict.items():\n",
    "    # Calculate descriptive statistics for the 'ret_excess' column\n",
    "    descriptives = round(data_df['ret_excess'].describe(), 3) * 100\n",
    "    # Store the descriptive statistics in the descriptives_dict\n",
    "    descriptives_dict[key] = descriptives\n",
    "\n",
    "# Convert the descriptives_dict to a DataFrame\n",
    "descriptives_df = pd.concat(descriptives_dict.values(), keys=descriptives_dict.keys(), axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "descriptives_df.columns = ['50', '100', '250', '500', '1000', '1500','2000']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "descriptives_df = descriptives_df.applymap(lambda x: f\"{x:.4f}\" if isinstance(x, float) else x) \n",
    "latex_table = descriptives_df.to_latex(float_format=\"%.2f\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e7fce-a2ed-4603-8a68-4d4ae4e81670",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives_df.iloc[0, :] = pd.to_numeric(descriptives_df.iloc[0, :], errors='coerce')\n",
    "\n",
    "# Multiply the \"count\" row by 11\n",
    "observations = descriptives_df.iloc[0, :] * 11 * 0.8\n",
    "\n",
    "# Print number of observations\n",
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46602380-c0cc-4d45-af88-36f238a6e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import * \n",
    "from mizani.formatters import percent_format, date_format\n",
    "from mizani.breaks import date_breaks\n",
    "def violin(data):\n",
    "    data_plot = (ggplot(data, \n",
    "                  aes(x=\"ticker\", y=\"ret_excess\")) + \n",
    "                  geom_violin(fill='deepskyblue', color = 'deepskyblue') + \n",
    "                  labs(x=\"\", y=\"\", \n",
    "                      title = \"\") +\n",
    "                  scale_y_continuous(labels=percent_format()) +\n",
    "                 theme_classic() + \n",
    "                 theme(figure_size=(10, 4),\n",
    "                        axis_text_x=element_text(rotation=45, hjust=1, size = 8),\n",
    "                        panel_grid_major=element_line(color=\"lightgray\", size=0.5),\n",
    "                        panel_grid_minor=element_blank())  +\n",
    "                geom_point(stat=\"summary\", fun_y=np.mean, size=0.7, color=\"red\")\n",
    "                )\n",
    "    data_plot.save(filename='plots/excess_return_distribution.png', format='png', verbose=False)\n",
    "    return data_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b100a-ffe0-49c2-bdf2-d631744a6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_des = data_dict['data_55']\n",
    "df_violin = df_des[df_des['ticker'].isin(df_des['ticker'].unique()[:35])]\n",
    "violin(df_violin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ee7bf-1b23-4dad-a178-242ae726c356",
   "metadata": {},
   "source": [
    "### Time-series plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9c491-95f9-4369-8850-0daba1cd22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excess returns\n",
    "df_des['month'] = pd.to_datetime(df_des['month'])\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Group by 'ticker' and plot 'ret_excess' for each group\n",
    "for ticker, group in df_des.groupby('ticker'):\n",
    "    plt.plot(group['month'], group['ret_excess'], label=ticker)\n",
    "plt.ylabel('Excess Return')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.grid(True, alpha=0.4)  # Add grid with more transparent lines\n",
    "plt.axvspan(pd.Timestamp('2017-07-01'), df_des['month'].max(), color='gray', alpha=0.2)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574512a-6595-4825-acad-04a5aad68d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro predictors\n",
    "macro_value1 = ['Month', 'Dividend- Price Ratio',\n",
    " 'Dividend Yield',\n",
    " 'Earnings Price Ratio',\n",
    " 'Dividend Payout Ratio',\n",
    " 'Stock Variance',\n",
    " 'Book-to-Market Ratio',\n",
    " 'Net Equity Expansion',\n",
    " 'Treasury Bills (3 month)',\n",
    " 'Long-Term Yield',\n",
    " 'Long-Term Rate of Return',\n",
    " 'Term Spread',\n",
    " 'Default Yield Spread','Inflation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f91f95-b875-4e86-b567-8caed0d019a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_predictors.columns = macro_value1\n",
    "macro_predictors.set_index('Month', inplace=False)\n",
    "\n",
    "# Get the list of macro predictor column names\n",
    "predictor_columns = macro_predictors.columns.tolist()[1:]\n",
    "predictor_columns.remove('Treasury Bills (3 month)')\n",
    "\n",
    "# Calculate the number of rows and columns needed\n",
    "num_plots = len(predictor_columns)\n",
    "num_rows = (num_plots + 1) // 2  # Add 1 and use integer division to round up\n",
    "num_cols = 2\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(9, 2 * num_rows))\n",
    "for i, predictor in enumerate(predictor_columns):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    ax = axes[row, col] if num_rows > 1 else axes[col]\n",
    "    ax.plot(macro_predictors['Month'], macro_predictors[predictor], color = \"deepskyblue\")\n",
    "    ax.set_title(predictor, size = 10)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('pct.')\n",
    "    ax.tick_params(axis='x', labelsize=8, rotation = 45)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    ax.grid(True)\n",
    "if num_plots % 2 != 0:\n",
    "    fig.delaxes(axes[-1, -1])\n",
    "plt.tight_layout()\n",
    "folder_name = \"plots\"\n",
    "plt.savefig(os.path.join(folder_name, \"macro_plots.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7b0ac-5c31-470b-87c8-4810a301e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fama-French Factors\n",
    "#factors_ff5_monthly.set_index('factor_ff5_month', inplace=False)\n",
    "factors_ff5_monthly.columns=['Month','Market Excess Return','Small-Minus-Big (Size)','High-Minus-Low (Value)','Robust-Minus-Weak (Profitability)','Conservative-Minus-Aggressive (Investment)','Risk-Free Interest']\n",
    "# Get the list of macro predictor column names\n",
    "predictor_columns_ff = factors_ff5_monthly.columns.tolist()[1:]\n",
    "\n",
    "# Calculate the number of rows and columns needed\n",
    "num_plots = len(predictor_columns_ff)\n",
    "num_rows = (num_plots + 1) // 2  # Add 1 and use integer division to round up\n",
    "num_cols = 2\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(9, 2 * num_rows))\n",
    "for i, predictor in enumerate(predictor_columns_ff):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    ax = axes[row, col] if num_rows > 1 else axes[col]\n",
    "    ax.plot(factors_ff5_monthly['Month'], factors_ff5_monthly[predictor], color = \"deepskyblue\")\n",
    "    ax.set_title(predictor, size = 10)\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('pct.', size = 8)\n",
    "    ax.tick_params(axis='x', labelsize=8, rotation = 45)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    ax.grid(True)\n",
    "if num_plots % 2 != 0:\n",
    "    fig.delaxes(axes[-1, -1])\n",
    "plt.tight_layout()\n",
    "folder_name = \"plots\"\n",
    "plt.savefig(os.path.join(folder_name, \"fama_plots.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
