{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847a53e-4e08-4ec4-9741-5b9dbefca0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requirements.txt\", \"r\") as config_file:\n",
    "    config_code = config_file.read()\n",
    "    exec(config_code)\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f785c82-7381-4619-82b1-384947d58a10",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3bd88-16e9-45cf-a459-4bb4c887f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_finance = sqlite3.connect(database=\"data/specialedata.sqlite\")\n",
    "\n",
    "macro_predictors = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM macro_predictors\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"macro_\")\n",
    ")\n",
    "\n",
    "JKPFactors = (pd.read_sql_query(\n",
    "  sql=\"SELECT * FROM JKPFactors\",\n",
    "  con=tidy_finance,\n",
    "  parse_dates={\"month\"})\n",
    "  .add_prefix(\"jkp_factor_\")\n",
    ")\n",
    "JKPFactornames = JKPFactors.columns\n",
    "\n",
    "factors_ff3_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff3_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff3_\")\n",
    ")\n",
    "\n",
    "ff_carhart = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM ff_carhart\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"ff_carhart_\")\n",
    ")\n",
    "\n",
    "factors_ff5_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff5_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff5_\")\n",
    ")\n",
    "crsp_2000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_2000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_2000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_250 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_250\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_100 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_100\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_50 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_50\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "# Select amount of tickers in cross section!\n",
    "data_total = (crsp_1000\n",
    "        .merge(JKPFactors,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"jkp_factor_month\")\n",
    "        .merge(macro_predictors,\n",
    "             how = \"left\", left_on = \"month\", right_on = \"macro_month\")\n",
    "        .merge(factors_ff5_monthly,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"factor_ff5_month\")\n",
    "         .merge(ff_carhart,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"ff_carhart_month\")\n",
    "        .assign(ret_excess=lambda x: x[\"ret\"] - x[\"factor_ff5_rf\"]) \n",
    "        .drop(columns=['ret', 'jkp_factor_month', 'macro_month', 'factor_ff5_month', 'factor_ff5_rf', 'ff_carhart_month','ff_carhart_rf','ff_carhart_mkt_excess','ff_carhart_smb','ff_carhart_hml'])\n",
    "        .dropna()\n",
    "       )\n",
    "\n",
    "# Make a dataframe for stock characteristics and factors\n",
    "macro_variables = data_total.filter(like=\"macro\").columns\n",
    "factor_variables = data_total.filter(like=\"jkp_factor\").columns\n",
    "macro_factors = data_total[macro_variables]\n",
    "factors = data_total[macro_variables].merge(data_total[factor_variables], left_index=True, right_index=True)\n",
    "char = data_total[['mktcap', 'mktcap_lag_1', 'mktcap_lag_3', 'mktcap_lag_6', 'mktcap_lag_12', 'mom_1', 'mom_3','mom_6', 'mom_12']]\n",
    "# List of tickers\n",
    "tickers = data_total['ticker'].unique()\n",
    "\n",
    "training_date = \"2017-07-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad3a08-a0d5-433d-86ea-cc37068a35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_total.copy()\n",
    "df = data.copy()\n",
    "df.set_index('month', inplace=True)\n",
    "y = df['ret_excess']\n",
    "X = df.drop(columns=['ret_excess', 'ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0eb7f4-dce8-4409-bb90-8548595a74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select prominent features\n",
    "X = X[['mktcap','mktcap_lag_1','mktcap_lag_3','mktcap_lag_6','mktcap_lag_12', 'mom_1', 'mom_3', 'mom_6', 'mom_12','volume','macro_lty','macro_bm','factor_ff5_mkt_excess','factor_ff5_smb','factor_ff5_hml','factor_ff5_rmw','factor_ff5_cma','ff_carhart_mom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b63a8-0f59-4349-903d-96ec72a0d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test samples\n",
    "scaler = StandardScaler()\n",
    "X_scaled_array = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled_array, index=X.index, columns=X.columns)\n",
    "\n",
    "# Training 80 pct.\n",
    "training_date = \"2017-07-01\"\n",
    "X_train = X_scaled[X.index < training_date]\n",
    "X_test = X_scaled[X.index >= training_date]\n",
    "y_train = y[y.index < training_date]\n",
    "y_test = y[y.index >= training_date]\n",
    "\n",
    "n_features = X_train.shape[1] # Number of features\n",
    "feature_names = df.drop(columns=['ticker','ret_excess']).columns.tolist() # Feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68131008-5e53-4f93-88d0-fb5e399c9f75",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6e341-919d-4064-9a00-68f1abaf2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numbers of estimators\n",
    "#num_estimators = [10, 25, 50, 100, 150, 200]\n",
    "num_estimators = [150, 200]\n",
    "\n",
    "# Create an empty dictionary to store the models\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "# Loop through the numbers of estimators\n",
    "for n in num_estimators:\n",
    "    start = time.time()\n",
    "\n",
    "    # Define and train the Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print(f'model_{n}: {round((end - start)/60,3)} minutes')\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    models[f'model_{n}'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf5d70-4a06-4cfd-ab60-c904e0648675",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    pl.RF_plot(models, model_name, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8e65d-ab8f-4176-9fa8-5064e88b0811",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b715c3-957a-485d-bcfe-ac4645fb3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Loop through the models\n",
    "for model_name, model in models.items():\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the evaluation results in the dictionary\n",
    "    evaluation_results[model_name] = {'Mean Squared Error': mse, 'R^2 Score': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c1446-78f9-4077-b018-7ae8cacd4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"Evaluation results for {model_name}:\")\n",
    "    #print(\"Mean Squared Error:\", round(results['Mean Squared Error'],4))\n",
    "    print(\"R^2 Score:\", round(results['R^2 Score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28c427-4a13-430a-866b-98129cc0ac08",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db9a6f-4490-4543-8b05-d77766708b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    pl.RF_feature_importances(model, X.columns, model_name, figsize=(7, 4), threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d739344-5383-4e92-876e-f1354678d9a3",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8b3bb-87a9-4b7a-911f-504ef266f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(layers, neurons, n_features):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(neurons[0], activation='relu', input_shape=(n_features,), name='input_layer'))\n",
    "    \n",
    "    for i in range(layers):\n",
    "        model.add(keras.layers.Dense(neurons[i], activation='relu', name=f'hidden_layer_{i+1}'))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1, name='output_layer'))  # Output layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25245dc-9cb6-4546-90f7-2fec4b45c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = create_neural_network(1, [8], n_features)\n",
    "model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_2 = create_neural_network(2, [8,4], n_features)\n",
    "model_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_3 = create_neural_network(3, [8,4,2], n_features)\n",
    "model_3.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_4 = create_neural_network(4, [8,4,2,1], n_features)\n",
    "model_4.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_5 = create_neural_network(5, [64,32,16,8,4], n_features)\n",
    "model_5.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_6 = create_neural_network(6, [64,32,16,8,4,2,1], n_features)\n",
    "model_6.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536fc32-13eb-4509-b170-315b871b734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(model, X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=50, verbose = 0,  batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R^2 score and MSPE\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    #mspe = mean_squared_error(y_test, y_pred)\n",
    "    print(\" Out-of-sample R^2:\", round(r2,4))\n",
    "    #print(\"Mean Squared Prediction Error (MSPE):\", round(mspe,4))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'{round(end - start,3)/60} minutes')\n",
    "models_NN = [model_1, model_2, model_3, model_4, model_5, model_6]\n",
    "model_names_NN = ['model_1', 'model_2', 'model_3', 'model_4', 'model_5', 'model_6']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4a2ae-84c6-4738-a5ef-63d1e68c67c6",
   "metadata": {},
   "source": [
    "### Evaluation and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7458607-371b-4320-9714-67b80acc9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_NN:\n",
    "    NN(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2ef7e-229a-4a7f-8092-d794949f0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_name in zip(models_NN, model_names_NN):\n",
    "    pl.NN_plot(model, model_name, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc48770-8725-415e-ae10-83752d5c65b8",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b644c-b936-4bdb-93cf-0b3c393772f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_name in zip(models_NN, model_names_NN):\n",
    "    pl.NN_feature_importance(model, feature_names, model_name, threshold=0.007, figsize=(7, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
