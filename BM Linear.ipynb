{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b51fdf-4f08-4ef7-95f4-d891571d159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requirements.txt\", \"r\") as config_file:\n",
    "    config_code = config_file.read()\n",
    "    exec(config_code)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.panel import PooledOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c18457-70c3-42c9-a88e-3580d8f519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_finance = sqlite3.connect(database=\"data/specialedata.sqlite\")\n",
    "\n",
    "macro_predictors = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM macro_predictors\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"macro_\")\n",
    ")\n",
    "\n",
    "JKPFactors = (pd.read_sql_query(\n",
    "  sql=\"SELECT * FROM JKPFactors\",\n",
    "  con=tidy_finance,\n",
    "  parse_dates={\"month\"})\n",
    "  .add_prefix(\"jkp_factor_\")\n",
    ")\n",
    "JKPFactornames = JKPFactors.columns\n",
    "\n",
    "factors_ff3_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff3_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff3_\")\n",
    ")\n",
    "\n",
    "factors_ff5_monthly = (pd.read_sql_query(\n",
    "     sql=\"SELECT * FROM factors_ff5_monthly\",\n",
    "     con=tidy_finance,\n",
    "     parse_dates={\"month\"})\n",
    "  .add_prefix(\"factor_ff5_\")\n",
    ")\n",
    "\n",
    "ff_carhart = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM ff_carhart\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    " .add_prefix(\"ff_carhart_\")\n",
    ")\n",
    "crsp_2000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_2000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_1000 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_1000\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_500 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_500\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "crsp_250 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_250\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_100 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_100\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "crsp_50 = (pd.read_sql_query(\n",
    "    sql=\"SELECT * FROM crsp_50\",\n",
    "    con=tidy_finance,\n",
    "    parse_dates={\"month\"})\n",
    ")\n",
    "\n",
    "# Select amount of tickers in cross section!\n",
    "data_total = (crsp_2000\n",
    "        .merge(factors_ff5_monthly,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"factor_ff5_month\")\n",
    "        .assign(ret_excess=lambda x: x[\"ret\"] - x[\"factor_ff5_rf\"]) \n",
    "        .drop(columns=['ret', 'factor_ff5_month'])\n",
    "        .dropna()\n",
    "       )\n",
    "\n",
    "# Make a dataframe for stock characteristics and factors\n",
    "macro_variables = data_total.filter(like=\"macro\").columns\n",
    "factor_variables = data_total.filter(like=\"jkp_factor\").columns\n",
    "macro_factors = data_total[macro_variables]\n",
    "factors = data_total[macro_variables].merge(data_total[factor_variables], left_index=True, right_index=True)\n",
    "char = data_total[['mktcap', 'mktcap_lag_1', 'mktcap_lag_3', 'mktcap_lag_6', 'mktcap_lag_12', 'mom_1', 'mom_3','mom_6', 'mom_12']]\n",
    "# List of tickers\n",
    "tickers = data_total['ticker'].unique()\n",
    "\n",
    "# Transform data\n",
    "column_combinations = list(product(macro_factors, char)) \n",
    "\n",
    "new_column_values = []\n",
    "for macro_column, char in column_combinations:\n",
    "    new_column_values.append(data_total[macro_column] * data_total[char])\n",
    "\n",
    "column_names = [\" x \".join(t) for t in column_combinations]\n",
    "new_columns = pd.DataFrame(dict(zip(column_names, new_column_values)))\n",
    "\n",
    "# New data set with added combinations\n",
    "data = pd.concat([data_total, new_columns], axis=1)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "  transformers=[\n",
    "    (\"scale\", StandardScaler(), \n",
    "    [col for col in data.columns \n",
    "      if col not in [\"ret_excess\", \"month\", \"ticker\"]])\n",
    "  ],\n",
    "  remainder=\"drop\",\n",
    "  verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "training_date = \"2017-07-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f001d-057f-49a5-b5c2-afee6c7b386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF 5 factor model\n",
    "data_ff5 = data[['month','ticker','ret_excess','factor_ff5_mkt_excess','factor_ff5_smb','factor_ff5_hml','factor_ff5_rmw','factor_ff5_cma']]\n",
    "colnames = ['month','ticker','ret_excess','mkt_excess','smb','hml','rmw','cma']\n",
    "data_ff5.columns=colnames\n",
    "# FF 3 factor model\n",
    "data_ff3 = data_ff5.iloc[:,:6]\n",
    "# CAPM\n",
    "data_capm = data_ff3.iloc[:,:4]\n",
    "# Carhart\n",
    "data_carhart = (data_ff3\n",
    "        .merge(ff_carhart,\n",
    "               how = \"left\", left_on = \"month\", right_on = \"ff_carhart_month\")\n",
    "        .drop(columns=['ff_carhart_mkt_excess','ff_carhart_month','ff_carhart_smb','ff_carhart_hml', 'ff_carhart_rf'])\n",
    "        .dropna()\n",
    "       )\n",
    "data_carhart.columns = ['month','ticker','ret_excess','mkt_excess','smb','hml','mom']\n",
    "\n",
    "# Set indeces\n",
    "data_ff5 = data_ff5.set_index([\"ticker\",\"month\"])\n",
    "data_ff3= data_ff3.set_index([\"ticker\",\"month\"])\n",
    "data_capm = data_capm.set_index([\"ticker\",\"month\"])\n",
    "data_carhart = data_carhart.set_index([\"ticker\",\"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8109a-f60d-4c8b-bcb1-4a27cad129c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    training_date = \"2017-07-01\"\n",
    "    # Step 1: Filter data until the training date\n",
    "    data_train = data[data.index.get_level_values('month') <= training_date]\n",
    "    data_test = data[data.index.get_level_values('month') > training_date]\n",
    "\n",
    "    # Step 2: Prepare exogenous and endogenous variables\n",
    "    exog_vars = data.columns[1:].tolist()\n",
    "    endog_vars = ['ret_excess']  # Corrected spelling\n",
    "    X_train = sm.add_constant(data_train[exog_vars])\n",
    "    y_train = data_train[endog_vars]\n",
    "    \n",
    "    X_test = data_test[exog_vars]\n",
    "    y_test = data_test[endog_vars]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "def R_oos(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test).flatten(), np.array(y_pred).flatten()\n",
    "    return 1 - (np.dot((y_test-y_pred),(y_test-y_pred)))/(np.dot(y_test,y_test))\n",
    "\n",
    "def estimate(data, X_train, y_train, X_test, y_test):\n",
    "    model = PooledOLS(y_train, X_train)\n",
    "    pooled_res = model.fit()\n",
    "    \n",
    "    data_forecast = data[data.index.get_level_values('month') > training_date]\n",
    "    exog_vars = data.columns[1:].tolist()\n",
    "    X_forecast = sm.add_constant(data_forecast[exog_vars])\n",
    "    y_fitted = pooled_res.fitted_values\n",
    "\n",
    "    y_pred = pooled_res.predict(X_forecast)\n",
    "    print(f'Out-of-sample R-squared: {round(R_oos(y_test,y_pred),4)}')\n",
    "    return y_pred, y_fitted\n",
    "\n",
    "def plot(data, y_pred, y_fitted, dataset_name):\n",
    "    training_date = \"2017-07-01\"\n",
    "    plt.figure(figsize=(8, 3))\n",
    "\n",
    "    # Sort the DataFrame by index\n",
    "    df_sorted = data.sort_index()\n",
    "    y_pred = y_pred.sort_index()\n",
    "    y_fitted = y_fitted.sort_index()\n",
    "    # Plot line plots first\n",
    "    for ticker in df_sorted.index.levels[0]:\n",
    "        data_ticker = df_sorted.loc[ticker]\n",
    "        plt.scatter(data_ticker.index.get_level_values('month'), data_ticker['ret_excess'], label=ticker, color='deepskyblue', s=5, zorder=10, marker='o')\n",
    "        plt.scatter(y_pred.loc[ticker].index.get_level_values('month'), y_pred.loc[ticker]['predictions'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "        plt.scatter(y_fitted.loc[ticker].index.get_level_values('month'), y_fitted.loc[ticker]['fitted_values'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "    \n",
    "    plt.axvspan(training_date, max(data.index.get_level_values('month')), color='gray', alpha=0.2)\n",
    "    plt.grid(color='lightgray', linewidth=0.5, alpha=0.5)\n",
    "    plt.title(f'{dataset_name.upper()}', size = 8)\n",
    "\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Excess Return')\n",
    "\n",
    "    # Save\n",
    "    save_dir = 'plots/bm'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = os.path.join(save_dir, f'{dataset_name.lower()}_plot.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fc16f-76bd-4788-8a32-1f31c183f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [data_capm, data_ff3, data_carhart, data_ff5]  # Assuming data_capm and data_ff5 are your datasets\n",
    "\n",
    "for i, data in enumerate(datasets):\n",
    "    # Create variable names based on dataset names\n",
    "    dataset_name = ['capm', 'ff3', 'carhart','ff5'][i]  # Assuming dataset names are 'capm' and 'ff5'\n",
    "    X_train_name = f'X_train_{dataset_name}'\n",
    "    y_train_name = f'y_train_{dataset_name}'\n",
    "    X_test_name = f'X_test_{dataset_name}'\n",
    "    y_test_name = f'y_test_{dataset_name}'\n",
    "    y_pred_name = f'y_pred_{dataset_name}'\n",
    "    y_fitted_name = f'y_fitted_{dataset_name}'\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, y_train, X_test, y_test = train_test_split(data)\n",
    "    \n",
    "    # Assign the train-test splits to the dynamically created variable names\n",
    "    globals()[X_train_name] = X_train\n",
    "    globals()[y_train_name] = y_train\n",
    "    globals()[X_test_name] = X_test\n",
    "    globals()[y_test_name] = y_test\n",
    "    print(f\"Model: {dataset_name}\")\n",
    "    # Run estimate function on the current dataset\n",
    "    y_pred, y_fitted = estimate(data, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Store y_pred and y_fitted using globals\n",
    "    globals()[y_pred_name] = y_pred\n",
    "    globals()[y_fitted_name] = y_fitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4537e-5038-4e98-bb0f-7b0e221e7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(data_capm, y_pred_capm, y_fitted_capm, 'capm'),\n",
    "            (data_ff3, y_pred_ff3, y_fitted_ff3, 'ff3'),\n",
    "            (data_ff5, y_pred_ff5, y_fitted_ff5, 'ff5'),\n",
    "           (data_carhart, y_pred_carhart, y_fitted_carhart, 'carhart')]\n",
    "\n",
    "for data, y_pred, y_fitted, dataset_name in datasets:\n",
    "    plot(data, y_pred, y_fitted, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972a3e6-ef92-4ca9-921d-ac1f9a596eb6",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cbf9e-cfee-40ce-956c-4fece858b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_regularization(data):\n",
    "    y = data['ret_excess']\n",
    "    X = data.drop(columns = ['ret_excess'])\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_array = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled_array, index = X.index, columns = X.columns)\n",
    "    # Training 80 pct.\n",
    "    training_date = \"2017-07-01\"\n",
    "    X_train = X_scaled[X.index.get_level_values('month') < training_date]\n",
    "    X_test = X_scaled[X.index.get_level_values('month') >= training_date]\n",
    "    y_train = y[y.index.get_level_values('month') < training_date]\n",
    "    y_test = y[y.index.get_level_values('month') >= training_date]\n",
    "    \n",
    "    n_features = X_train.shape[1] # Number of features\n",
    "    feature_names = data.drop(columns=['ret_excess']).columns.tolist() # Feature names\n",
    "    \n",
    "    n_features = X_train.shape[1] # Number of features\n",
    "    feature_names = data.drop(columns=['ret_excess']).columns.tolist() # Feature names\n",
    "    y_train_ = np.array(y_train).reshape(-1,1)\n",
    "    return X_train, X_test, y_train, y_test, y_train_, n_features, feature_names\n",
    "\n",
    "def optimal_alpha_EN(X_train,y_train_, l1_ratio):\n",
    "    alphas = np.logspace(-6, 6, 100)\n",
    "    \n",
    "    lm_model = ElasticNet(alpha=0.006,\n",
    "                          l1_ratio=l1_ratio,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=lm_model, param_grid={'alpha': alphas}, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train.values, y_train_)\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "    best_alpha_str = \"{:.10f}\".format(round(best_alpha, 10))\n",
    "    \n",
    "    # Print the rounded alpha value\n",
    "    print(f'Optimal Alpha: {best_alpha_str}')\n",
    "    return best_alpha\n",
    "\n",
    "def optimal_alpha_Ridge(X_train,y_train_):\n",
    "    alphas = np.logspace(-6, 6, 100)\n",
    "    \n",
    "    lm_model = Ridge(alpha=0.006,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=lm_model, param_grid={'alpha': alphas}, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train.values, y_train_)\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "    best_alpha_str = \"{:.10f}\".format(round(best_alpha, 10))\n",
    "    \n",
    "    # Print the rounded alpha value\n",
    "    print(f'Optimal Alpha: {best_alpha_str}')\n",
    "    return best_alpha\n",
    "\n",
    "def EN_predict(X_train, y_train_, X_test, y_test, l1_ratio, alpha_opt):\n",
    "    lm_model = ElasticNet(alpha=alpha_opt,\n",
    "                          l1_ratio=l1_ratio,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    \n",
    "    lm_fit = lm_model.fit(X_train.values, y_train_)\n",
    "    predictions = lm_fit.predict(X_test.values)\n",
    "    fitted = lm_fit.predict(X_train.values)\n",
    "    \n",
    "    y_pred = pd.DataFrame(predictions, index = y_test.index)\n",
    "    y_fitted = pd.DataFrame(fitted, index = y_train.index)\n",
    "    \n",
    "    y_pred.columns = ['predictions']\n",
    "    y_fitted.columns = ['fitted_values']\n",
    "    print(f'Out-of-sample R-squared =', round(R_oos(y_test, y_pred),4))\n",
    "    return y_pred, y_fitted\n",
    "\n",
    "def Ridge_predict(X_train, y_train_, X_test, y_test, alpha_opt):\n",
    "    lm_model = Ridge(alpha=alpha_opt,\n",
    "                          max_iter=1000,\n",
    "                          fit_intercept=False)\n",
    "    \n",
    "    lm_fit = lm_model.fit(X_train.values, y_train_)\n",
    "    predictions = lm_fit.predict(X_test.values)\n",
    "    fitted = lm_fit.predict(X_train.values)\n",
    "    \n",
    "    y_pred = pd.DataFrame(predictions, index = y_test.index)\n",
    "    y_fitted = pd.DataFrame(fitted, index = y_train.index)\n",
    "    \n",
    "    y_pred.columns = ['predictions']\n",
    "    y_fitted.columns = ['fitted_values']\n",
    "    print(f'Out-of-sample R-squared =', round(R_oos(y_test, y_pred),4))\n",
    "    return y_pred, y_fitted\n",
    "def plot(data, y_pred, y_fitted, dataset_name, regularizor):\n",
    "    training_date = \"2017-07-01\"\n",
    "    plt.figure(figsize=(8, 3))\n",
    "\n",
    "    # Sort the DataFrame by index\n",
    "    df_sorted = data.sort_index()\n",
    "    y_pred = y_pred.sort_index()\n",
    "    y_fitted = y_fitted.sort_index()\n",
    "    # Plot line plots first\n",
    "    for ticker in df_sorted.index.levels[0]:\n",
    "        data_ticker = df_sorted.loc[ticker]\n",
    "        plt.plot(data_ticker.index.get_level_values('month'), data_ticker['ret_excess'], label=ticker, color='deepskyblue')\n",
    "        plt.scatter(y_pred.loc[ticker].index.get_level_values('month'), y_pred.loc[ticker]['predictions'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "        plt.scatter(y_fitted.loc[ticker].index.get_level_values('month'), y_fitted.loc[ticker]['fitted_values'], label=ticker, color='red', s=5, zorder=10, marker='o')\n",
    "    \n",
    "    plt.axvspan(training_date, max(data.index.get_level_values('month')), color='gray', alpha=0.2)\n",
    "    plt.grid(color='lightgray', linewidth=0.5, alpha=0.5)\n",
    "    plt.title(f'{dataset_name.upper()}', size = 8)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Excess Return')\n",
    "\n",
    "    # Save\n",
    "    save_dir = 'plots/bm/regularization'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = os.path.join(save_dir, f'{dataset_name.lower()}_plot_{regularizor}.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e5364-77c3-4fdb-b323-c1356035e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [data_capm, data_ff3, data_carhart, data_ff5]  # Assuming data_capm and data_ff5 are your datasets\n",
    "\n",
    "for i, data in enumerate(datasets):\n",
    "    # Create variable names based on dataset names\n",
    "    dataset_name = ['capm', 'ff3', 'carhart','ff5'][i]  # Assuming dataset names are 'capm' and 'ff5'\n",
    "    X_train_name = f'X_train_{dataset_name}'\n",
    "    y_train_name = f'y_train_{dataset_name}'\n",
    "    X_test_name = f'X_test_{dataset_name}'\n",
    "    y_test_name = f'y_test_{dataset_name}'\n",
    "    y_train__name = f'y_train__{dataset_name}'\n",
    "    n_features_name = f'n_features_{dataset_name}'\n",
    "    feature_names_name = f'n_feature_names_{dataset_name}' \n",
    "    y_pred_name = f'y_pred_{dataset_name}'\n",
    "    y_fitted_name = f'y_fitted_{dataset_name}'\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data)\n",
    "\n",
    "    # Assign the train-test splits to the dynamically created variable names\n",
    "    globals()[X_train_name] = X_train\n",
    "    globals()[y_train_name] = y_train\n",
    "    globals()[X_test_name] = X_test\n",
    "    globals()[y_test_name] = y_test\n",
    "    globals()[y_train__name] = y_train_\n",
    "    globals()[n_features_name] = n_features\n",
    "    globals()[feature_names_name] = feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9146b-acb3-40c6-9e38-86fec10348c7",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5ea76-dd5a-4bf7-b7dd-e9df2066823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAPM\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_capm)\n",
    "alpha_opt = optimal_alpha_EN(X_train_capm,y_train__capm,1)\n",
    "y_pred, y_fitted = EN_predict(X_train_capm, y_train__capm, X_test_capm,y_test_capm, 1, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7866ad6-7681-49c5-9c5b-744f8da7828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff3\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_ff3)\n",
    "alpha_opt = optimal_alpha_EN(X_train,y_train_,1)\n",
    "y_pred, y_fitted = EN_predict(X_train, y_train_, X_test,y_test, 1, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fef2e-aca5-47ec-8301-e4658e269965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carhart\n",
    "X_train_carhart, X_test_carhart, y_train_carhart, y_test_carhart, y_train__carhart, n_features_carhart, feature_names_carhart = train_test_split_regularization(data_carhart)\n",
    "alpha_opt = optimal_alpha_EN(X_train,y_train_,1)\n",
    "y_pred_carhart, y_fitted = EN_predict(X_train, y_train_, X_test,y_test, 1, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b3aa3-da50-455c-b1a9-0b2f3b05949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff5\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_ff5)\n",
    "alpha_opt = optimal_alpha_EN(X_train,y_train_,1)\n",
    "y_pred, y_fitted = EN_predict(X_train, y_train_, X_test,y_test, 1, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3ca1e-2193-43d0-9444-2c05681dbc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(data_capm, y_pred_capm, y_fitted_capm, dataset_name = 'CAPM', regularizor = 'Lasso')\n",
    "#plot(data_ff3, y_pred_ff3, y_fitted_ff3, dataset_name = 'FF3', regularizor = 'Lasso')\n",
    "#plot(data_ff5, y_pred_ff5, y_fitted_ff5, dataset_name = 'FF5', regularizor = 'Lasso')\n",
    "#plot(data_carhart, y_pred_carhart, y_fitted_carhart, dataset_name = 'Carhart', regularizor = 'Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c174e03-92a4-480b-83a6-28a53b851909",
   "metadata": {},
   "source": [
    "### EN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110045e-a664-488e-afb1-071761b9507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAPM\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_capm)\n",
    "alpha_opt = optimal_alpha_EN(X_train,y_train_,0.5)\n",
    "y_pred, y_fitted = EN_predict(X_train, y_train_, X_test,y_test, 0.5, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca2270-7553-408e-986d-896266b0ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff3\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_ff3)\n",
    "alpha_opt = optimal_alpha_EN(X_train,y_train_,0.5)\n",
    "y_pred, y_fitted = EN_predict(X_train, y_train_, X_test,y_test, 0.5, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f4b7b-ca82-48b8-bba5-958f325a2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carhart\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_carhart)\n",
    "alpha_opt = optimal_alpha_EN(X_train,y_train_,0.5)\n",
    "y_pred, y_fitted = EN_predict(X_train, y_train_, X_test,y_test, 0.5, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5fa85-fd26-4a9a-8b49-fda316f094dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff5\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_ff5)\n",
    "alpha_opt = optimal_alpha_EN(X_train,y_train_,0.5)\n",
    "y_pred, y_fitted = EN_predict(X_train, y_train_, X_test,y_test, 0.5, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bc754-d38c-4817-9e0e-d7e2a4860a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(data_capm, y_pred, y_fitted, dataset_name = '', regularizor = 'EN')\n",
    "#plot(data_ff3, y_pred, y_fitted, dataset_name = '', regularizor = 'EN')\n",
    "#plot(data_ff5, y_pred, y_fitted, dataset_name = '', regularizor = 'EN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1df3b-3597-4256-8431-63d9d7a5872e",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d9f62-fc62-46b5-af22-508c98c5ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAPM\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_capm)\n",
    "alpha_opt = optimal_alpha_Ridge(X_train,y_train_)\n",
    "y_pred, y_fitted = Ridge_predict(X_train, y_train_, X_test,y_test, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308cb1f-327a-4584-92e8-614aec7eb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff3\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_ff3)\n",
    "alpha_opt = optimal_alpha_Ridge(X_train,y_train_)\n",
    "y_pred, y_fitted = Ridge_predict(X_train, y_train_, X_test,y_test, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ec188-a1b9-4e9f-8799-742a8fc7c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carhart\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_carhart)\n",
    "alpha_opt = optimal_alpha_Ridge(X_train,y_train_)\n",
    "y_pred, y_fitted = Ridge_predict(X_train, y_train_, X_test,y_test, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf6862-03b8-48ca-a4c0-b4c429f9108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff5\n",
    "X_train, X_test, y_train, y_test, y_train_, n_features, feature_names = train_test_split_regularization(data_ff5)\n",
    "alpha_opt = optimal_alpha_Ridge(X_train,y_train_)\n",
    "y_pred, y_fitted = Ridge_predict(X_train, y_train_, X_test,y_test, alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3dc00-6df5-47d0-aaec-ab31af8897d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(data_capm, y_pred, y_fitted, dataset_name = '', regularizor = 'Ridge')\n",
    "#plot(data_ff3, y_pred, y_fitted, dataset_name = '', regularizor = 'Ridge')\n",
    "#plot(data_ff5, y_pred, y_fitted, dataset_name = '', regularizor = 'Ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34a196-5b59-4c3d-8ee5-d8c194d8eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_selected_features(lm_pipeline, finder, data):\n",
    "    # Fit the pipeline to the data\n",
    "    lm_pipeline.fit(data.drop(columns=[\"ret_excess\"]), data[\"ret_excess\"])\n",
    "\n",
    "    # Extract selected features based on the type of regression\n",
    "    if 'lasso' in lm_pipeline.named_steps['regressor'].__class__.__name__.lower() or 'elasticnet' in lm_pipeline.named_steps['regressor'].__class__.__name__.lower():\n",
    "        selected_features = pd.DataFrame(\n",
    "            finder.best_estimator_.named_steps.regressor.coef_ != 0,\n",
    "            index=lm_pipeline[:-1].named_steps.preprocessor.get_feature_names_out(),\n",
    "            columns=[\"selected\"]\n",
    "        )\n",
    "    elif 'ridge' in lm_pipeline.named_steps['regressor'].__class__.__name__.lower():\n",
    "        selected_features = pd.DataFrame(\n",
    "            np.abs(finder.best_estimator_.named_steps.regressor.coef_) > 0.05, # Absolute value \n",
    "            index=lm_pipeline[:-1].named_steps.preprocessor.get_feature_names_out(),\n",
    "            columns=[\"selected\"]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported regression type. Supported types: Lasso, Elastic Net, Ridge\")\n",
    "    \n",
    "    # Remove prefixes from variable names if necessary\n",
    "    selected_features.index = selected_features.index.str.replace(\"factor_|ff_|q_|macro_\", \"\")\n",
    "\n",
    "    return selected_features.astype(bool)\n",
    "\n",
    "factors_Lasso = extract_selected_features(lm_pipeline_Lasso, finder_Lasso, data)\n",
    "factors_Ridge = extract_selected_features(lm_pipeline_Ridge, finder_Ridge, data)\n",
    "factors_EN = extract_selected_features(lm_pipeline_EN, finder_EN, data)\n",
    "selected_factors = pd.concat([factors_Lasso,factors_Ridge,factors_EN], axis = 1)\n",
    "selected_factors.reset_index(inplace=True)\n",
    "selected_factors.columns = ['Factor', 'Lasso','Ridge','Elastic Net'] \n",
    "melted_factors = pd.melt(selected_factors, id_vars='Factor', var_name='Estimator', value_name='Selected')\n",
    "melted_factors = melted_factors[melted_factors['Selected']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
